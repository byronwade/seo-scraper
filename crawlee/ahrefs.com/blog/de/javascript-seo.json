{
  "url": "https://ahrefs.com/blog/de/javascript-seo/",
  "slug": "javascript-seo",
  "title": "JavaScript SEO: Was du wissen musst",
  "description": "Erfahre, wie Google mit JavaScript umgeht und wie du Probleme mit JavaScript SEO beheben kannst.",
  "content": "Patrick Stox Patrick Stox ist Produktberater, technischer SEO und Markenbotschafter bei Ahrefs. Er ist Organisator des SEO-Meetups in Raleigh, der SEO-Konferenz in Raleigh, des Beer & SEO-Meetups, der Findability-Konferenz und Moderator bei /r/TechSEO. Article PerformanceData from AhrefsVerlinkung von Websites 3 Sign up for AhrefsGet SEO metrics of any website or URL.The number of websites linking to this post.This post's estimated monthly organic search traffic. Inhalt Wusstest du, dass der Ahrefs-Blog zwar mit WordPress betrieben wird, der Rest der Website jedoch weitgehend mit JavaScript wie React betrieben wird?Die meisten Websites verwenden irgendeine Form von JavaScript, um Interaktivit√§t zu erm√∂glichen und die Benutzerfreundlichkeit zu verbessern. Einige verwenden es f√ºr Men√ºs, das Einblenden von Produkten oder Preisen, das Abrufen von Inhalten aus verschiedenen Quellen oder in einigen F√§llen f√ºr alle Elemente auf der Website. Die Realit√§t im modernen Internet ist, dass JavaScript allgegenw√§rtig ist.Wie John Mueller von Google sagte:The web has moved from plain HTML ‚Äî as an SEO you can embrace that. Learn from JS devs & share SEO knowledge with them. JS‚Äôs not going away.‚Äî üçå John üçå (@JohnMu) August 8, 2017Ich sage nicht, dass SEOs losziehen und lernen m√ºssen, wie man JavaScript programmiert. Das genaue Gegenteil ist der Fall. SEOs m√ºssen vor allem wissen, wie Google mit JavaScript umgeht und wie sie Probleme beheben k√∂nnen. In sehr wenigen F√§llen darf ein SEO den Code auch nur ber√ºhren. Mein Ziel mit diesem Beitrag ist es, dir beim Lernen zu helfen:Was ist Javascript SEOWie Google Seiten mit JavaScript verarbeitetWie man JavaScript testet und Fehler behebtRendering OptionenDeine JavaScript-Website SEO-freundlich gestaltenWas ist Javascript SEO?JavaScript SEO ist ein Teil des technischen SEO (Search Engine Optimization), der darauf abzielt, JavaScript-lastige Websites einfach zu crawlen und zu indizieren, sowie suchfreundlich zu machen. Das Ziel ist es, dass diese Websites in Suchmaschinen gefunden werden und ein h√∂heres Ranking erhalten.Ist JavaScript schlecht f√ºr SEO; ist JavaScript b√∂se? Ganz und gar nicht. Es ist nur anders als das, was viele SEOs gewohnt sind, und es gibt eine gewisse Lernkurve. Die Leute neigen dazu, es f√ºr Dinge zu nutzen, f√ºr die es wahrscheinlich eine bessere L√∂sung gibt. Aber manchmal muss man mit dem arbeiten, was man hat. Du musst nur wissen, dass Javascript nicht perfekt ist und nicht immer das richtige Werkzeug f√ºr die Aufgabe ist. Im Gegensatz zu HTML und CSS kann es nicht progressiv geparst werden, und es kann die Ladezeit der Seite und die Performance stark beeintr√§chtigen. In vielen F√§llen tauschst du m√∂glicherweise Leistung gegen Funktionalit√§t.Wie Google Seiten mit JavaScript verarbeitetIn den fr√ºhen Tagen der Suchmaschinen reichte eine heruntergeladene HTML-Antwort aus, um den Inhalt der meisten Seiten zu sehen. Dank des Vormarsches von JavaScript m√ºssen Suchmaschinen heute viele Seiten wie ein Browser darstellen, damit sie den Inhalt so sehen k√∂nnen, wie ein Benutzer ihn sieht.Das System, das den Rendering-Prozess bei Google abwickelt, hei√üt Web Rendering Service (WRS). Google hat ein vereinfachtes Diagramm zur Verf√ºgung gestellt, um zu zeigen, wie dieser Prozess funktioniert.Nehmen wir an, wir beginnen den Prozess bei URL.1. CrawlerDer Crawler sendet GET-Anforderungen an den Server. Der Server antwortet mit Headern und dem Inhalt der Datei, die dann gespeichert wird.Die Anfrage wird wahrscheinlich von einem mobilen Nutzer-Agenten kommen, da Google jetzt haupts√§chlich auf Mobile First indexiert ist. Mit dem URL-Inspektionstool in der Suchkonsole kannst du √ºberpr√ºfen, wie Google deine Website crawlt. Wenn du dies f√ºr eine URL ausf√ºhrst, √ºberpr√ºfe die Abdeckungsinformationen f√ºr ‚ÄúGecrawlt als‚Äù. Daran solltest du erkennen, ob du dich noch immer in der Desktop-Indizierung oder in der ‚ÄûMobile First‚Äú-Indizierung befindest.Die Anfragen kommen haupts√§chlich aus Mountain View, CA, USA, aber sie crawlen auch einige lokal angepassten Seiten au√üerhalb der Vereinigten Staaten. Ich erw√§hne dies, weil einige Websites Besucher aus einem bestimmten Land oder die Verwendung einer bestimmten IP auf verschiedene Weise blockieren oder behandeln, was dazu f√ºhren kann, dass dein Content vom Googlebot nicht gesehen wird.Einige Websites verwenden m√∂glicherweise auch die User-Agent-Erkennung, um einem bestimmten Crawler Inhalte anzuzeigen. Insbesondere bei JavaScript-Websites sieht Google m√∂glicherweise etwas anderes als ein Nutzer. Aus diesem Grund sind Google-Tools wie das URL-Inspektionstool in der Google-Suchkonsole, der Test zur Mobilfreundlichkeit und der Test zur Ermittlung von Rich Results wichtig f√ºr die Fehlerbehebung bei JavaScript-SEO-Problemen. Sie zeigen dir, was Google sieht, und sind n√ºtzlich, um zu √ºberpr√ºfen, ob Google m√∂glicherweise blockiert wird und ob sie den Inhalt auf der Seite sehen k√∂nnen. Wie du dies testen kannst, werde ich im Abschnitt √ºber den Renderer behandeln, da es einige wesentliche Unterschiede zwischen der heruntergeladenen GET-Anfrage, der gerenderten Seite und sogar den Testtools gibt.Es ist auch wichtig zu beachten, dass Google zwar die Ausgabe des Crawling-Vorgangs als ‚ÄûHTML‚Äú auf dem Bild oben angibt, in Wirklichkeit aber alle Ressourcen, die zum Erstellen der Seite ben√∂tigt werden, crawlt und speichert. HTML-Seiten, Javascript-Dateien, CSS, XHR-Anforderungen, API-Endpunkte und mehr.2. VerarbeitungEs gibt viele Systeme, die durch den Begriff ‚ÄûVerarbeitung‚Äú im Bild verschleiert werden. Ich werde einige davon behandeln, die f√ºr JavaScript relevant sind.Ressourcen und LinksGoogle navigiert nicht von Seite zu Seite, wie es ein Nutzer tun w√ºrde. Ein Teil der Verarbeitung besteht darin, die Seite auf Links zu anderen Seiten und Dateien zu √ºberpr√ºfen, die zum Aufbau der Seite ben√∂tigt werden. Diese Links werden herausgezogen und der Crawl-Warteschlange hinzugef√ºgt, die von Google zur Priorisierung und Planung des Crawlens verwendet wird.Google zieht Ressourcen-Links (CSS, JS usw.), die zum Aufbau einer Seite ben√∂tigt werden, aus Dingen wie <link> tags. Links zu anderen Seiten m√ºssen jedoch in einem bestimmten Format vorliegen, damit Google sie als Links behandeln kann. Interne und externe Links m√ºssen ein <a> tag mit einem href ‚ÄëAttribut aufweisen. Es gibt viele M√∂glichkeiten, wie du dies f√ºr Benutzer mit JavaScript umsetzen kannst, die nicht suchfreundlich sind.Gut:<a href=‚Äù/seite‚Äù>einfach ist gut</a> <a href=‚Äù/seite‚Äù onclick=‚ÄùgoTo(‚Äòseite‚Äô)‚Äù>immer noch okay</a> Schlecht:<a onclick=‚ÄùgoTo(‚Äòpage‚Äô)‚Äù>nein, kein href</a> <a href=‚Äùjavascript:goTo(‚Äòpage‚Äô)‚Äù>nein, Link fehlt</a> <a href=‚Äùjavascript:void(0)‚Äù>nein, Link fehlt</a> <span onclick=‚ÄùgoTo(‚Äòpage‚Äô)‚Äù>nicht das korrekte HTML-Element</span> <option value=\"page\">nein, falsches HTML-Element</option> <a href=‚Äù#‚Äù>kein Link</a> Button, ng-click, es gibt viele weitere M√∂glichkeiten, wie dies falsch gemacht werden kann. Es ist auch erw√§hnenswert, dass interne Links, die mit JavaScript hinzugef√ºgt wurden, erst nach dem Rendern √ºbernommen werden. Das sollte relativ schnell gehen und in den meisten F√§llen kein Grund zur Besorgnis sein.CachingJede Datei, die Google herunterl√§dt, einschlie√ülich HTML-Seiten, JavaScript-Dateien, CSS-Dateien usw., wird aggressiv gecached. Google ignoriert deine Cache-Timings und holt sich eine neue Kopie, wenn es das m√∂chte. Ich werde im Abschnitt Renderer ein wenig mehr dar√ºber sprechen und warum dies wichtig ist.Beseitigung von DuplikatenDoppelte Inhalte k√∂nnen aus dem heruntergeladenen HTML eliminiert oder depriorisiert werden, bevor sie zum Rendern gesendet werden. Bei App-Shell-Modellen kann es vorkommen, dass sehr wenig Inhalt und Code in der HTML-Antwort angezeigt wird. Tats√§chlich kann es vorkommen, dass auf jeder Seite der Website derselbe Code angezeigt wird, und dies k√∂nnte derselbe Code sein, der auf mehreren Websites angezeigt wird. Dies kann manchmal dazu f√ºhren, dass Seiten als Duplikate behandelt werden und nicht sofort zum Rendering geschickt werden. Schlimmer noch, die falsche Seite oder sogar die falsche Website kann in den Suchergebnissen angezeigt werden. Dies sollte sich mit der Zeit von selbst beheben, kann aber problematisch sein, insbesondere bei neueren Websites.Die restriktivsten RichtlinienGoogle w√§hlt die restriktivsten Anweisungen zwischen HTML und der gerenderten Version einer Seite aus. Wenn JavaScript eine Anweisung √§ndert und dies mit der Anweisung aus HTML in Konflikt steht, wird Google einfach diejenige Anweisung befolgen, die am restriktivsten ist. Noindex √ºberschreibt den Index, und Noindex in HTML √ºberspringt die Darstellung ganz.3. Render ListeJede Seite geht jetzt zum Renderer. Eine der gr√∂√üten Bedenken vieler SEOs mit JavaScript und zweistufiger Indizierung (HTML, dann gerenderte Seite) ist, dass Seiten m√∂glicherweise tage- oder sogar wochenlang nicht gerendert werden. Als Google dies untersuchte, stellte sich heraus, dass die Seiten bei einer durchschnittlichen Zeit von 5 Sekunden an den Renderer gingen, und das 90ste Perzentil lag bei Minuten. Die Zeitspanne zwischen dem Abrufen der HTML-Datei und dem Rendern der Seiten sollte also in den meisten F√§llen kein Problem darstellen.4. RendererDer Renderer ist der Ort, an dem Google eine Seite rendert, um zu sehen, was ein Nutzer sieht. Hier werden das JavaScript und alle von JavaScript am Document Object Model (DOM) vorgenommenen √Ñnderungen verarbeitet.Dazu verwendet Google einen headless Chrome-Browser, der jetzt ‚Äûevergreen‚Äú ist, d.h. er sollte die neueste Chrome-Version verwenden und die neuesten Funktionen unterst√ºtzen. Bis vor kurzem hat Google mit Chrome 41 gerendert, so dass viele Funktionen nicht unterst√ºtzt wurden.Google verf√ºgt √ºber weitere Informationen zum Web Rendering Service (WRS), der Dinge wie die Verweigerung von Berechtigungen, stateless, flattening-light-DOM und shadow-DOM und vieles mehr enth√§lt, was lesenswert ist. DOM, and more that is worth reading.Rendering im Ma√üstab des Web k√∂nnte das 8. Weltwunder sein. Es ist ein ernsthaftes Unterfangen und erfordert einen enormen Aufwand an Ressourcen. Wegen des Umfangs nimmt Google viele Abk√ºrzungen beim Rendering-Prozess, um die Dinge zu beschleunigen. Wir bei Ahrefs sind das einzige gro√üe SEO-Tool, das Webseiten im gro√üen Ma√üstab rendert, und wir schaffen es, ~150 Millionen Seiten pro Tag zu rendern, um unseren Link-Index vollst√§ndiger zu machen. Es erlaubt uns, auf JavaScript-Redirects zu pr√ºfen, und wir k√∂nnen auch Links anzeigen, die wir mit JavaScript eingef√ºgt gefunden haben und die wir mit einem JS-Tag in den Linkberichten anzeigen: Gecachte ResourcenGoogle verl√§sst sich stark auf gecachte Ressourcen. Seiten werden gecached; Dateien werden gecached; API-Anforderungen werden gecached; im Grunde wird alles im Cache gespeichert, bevor es an den Renderer gesendet wird. Google zieht nicht los und l√§dt bei jedem Seitenaufruf jede Ressource herunter, sondern verwendet stattdessen gecachte Ressourcen, um diesen Prozess zu beschleunigen.Dies kann zu einigen unm√∂glichen Zust√§nden f√ºhren, in denen fr√ºhere Dateiversionen im Rendering-Prozess verwendet werden und die indizierte Version einer Seite Teile √§lterer Dateien enthalten kann. Du kannst Dateiversionierung oder Content-Fingerprinting verwenden, um neue Dateinamen zu generieren, wenn signifikante √Ñnderungen vorgenommen werden, so dass Google die aktualisierte Version der Ressource zum Rendern herunterladen muss.Kein festes TimeoutEin verbreiteter SEO-Mythos ist, dass der Renderer nur f√ºnf Sekunden wartet, um deine Seite zu laden. Es ist zwar immer eine gute Idee, deine Seite schneller zu machen, aber dieser Mythos macht nicht wirklich Sinn mit der Art und Weise, wie Google die oben erw√§hnten Dateien zwischenspeichert. Im Grunde laden sie eine Seite, auf der bereits alles zwischengespeichert ist. Der Mythos r√ºhrt von den Test-Tools wie dem URL-Inspektionstool her, bei dem Ressourcen live abgerufen werden und sie ein vern√ºnftiges Limit setzen m√ºssen.Es gibt kein festes Timeout f√ºr den Renderer. Was sie wahrscheinlich tun, ist etwas √Ñhnliches wie das √∂ffentliche Rendertron. Sie warten wahrscheinlich auf etwas wie networkidle0, wo keine Netzwerkaktivit√§t mehr stattfindet, und setzen auch eine maximale Zeitspanne fest, falls etwas stecken bleibt oder jemand versucht, Bitcoin auf ihren Seiten abzubauen.Was Googlebot siehtGooglebot wird auf Webseiten nicht aktiv. Er wird nicht auf Dinge klicken oder scrollen, aber das bedeutet nicht, dass es keine Workarounds gibt. Was den Inhalt betrifft, so sehen sie ihn, solange er im DOM geladen ist, ohne dass eine Aktion erforderlich ist. Darauf werde ich im Abschnitt √ºber die Fehlerbehebung n√§her eingehen, aber im Grunde genommen wird der Inhalt, wenn er sich im DOM befindet, aber nur versteckt ist, gesehen. Wenn er erst nach einem Klick in das DOM geladen wird, dann wird der Inhalt nicht gefunden.Google muss auch nicht scrollen, um deinen Content zu sehen, weil es eine clevere Workaround-L√∂sung hat, um den Content zu sehen. F√ºr Handys laden sie die Seite mit einer Bildschirmgr√∂√üe von 411x731 Pixeln und passen die L√§nge auf 12.140 Pixel an. Im Wesentlichen wird daraus ein wirklich langes Telefon mit einer Bildschirmgr√∂√üe von 411x12140 Pixeln. F√ºr den Desktop machen sie dasselbe und gehen von 1024x768 Pixel auf 1024x9307 Pixel.Eine weitere interessante Abk√ºrzung ist, dass Google die Pixel w√§hrend des Rendervorgangs nicht darstellt. Das braucht Zeit und zus√§tzliche Ressourcen, um eine Seite fertig zu laden, und sie brauchen nicht wirklich den Endzustand mit den dargestellten Pixeln zu sehen. Sie m√ºssen nur die Struktur und das Layout kennen, und das bekommen sie, ohne dass sie die Pixel tats√§chlich darstellen m√ºssen. Wie Martin Splitt von Google es ausdr√ºckt:https://youtube.com/watch?v=Qxd_d9m9vzo%3Fstart%3D154Bei der Google-Suche k√ºmmern wir uns nicht wirklich um die Pixel, weil wir sie nicht wirklich jemandem zeigen wollen. Wir wollen die Informationen und die semantischen Informationen verarbeiten, also brauchen wir etwas im Zwischenzustand. Wir m√ºssen die Pixel nicht wirklich darstellen.Eine visuelle Darstellung k√∂nnte helfen, den herausgeschnittenen Teil etwas besser zu erkl√§ren. Wenn du in Chrome Dev Tools einen Test auf der Registerkarte Leistung ausf√ºhrst, erh√§ltst du ein Ladediagramm. Der durchgehend gr√ºne Teil stellt hier die Darstellungsphase dar, und f√ºr Googlebot geschieht das nie, so dass sie Ressourcen sparen.Grau = Downloads Blau = HTML Gelb = JavaScriptViolett = LayoutGr√ºn = Darstellung5. Crawl ListeGoogle hat eine Ressource, die ein wenig √ºber das Crawl-Budget spricht, aber du solltest wissen, dass jede Website ein eigenes Crawl-Budget hat und jede Anfrage priorisiert werden muss. Google muss au√üerdem das Crawling deiner Website im Vergleich zu jeder anderen Website im Internet ausgleichen. Neuere Websites im Allgemeinen oder Websites mit vielen dynamischen Seiten werden wahrscheinlich langsamer gecrawlt. Einige Seiten werden weniger h√§ufig aktualisiert als andere, und einige Ressourcen werden m√∂glicherweise auch weniger h√§ufig angefordert.Test/FehlerbehebungEin Haken bei JavaScript-Seiten ist, dass sie nur Teile des DOM aktualisieren k√∂nnen. Als Nutzer zu einer anderen Seite zu browsen, kann dazu f√ºhren, dass einige Aspekte wie Titel-Tags oder kanonische Tags im DOM nicht aktualisiert werden. Aber dies ist f√ºr Suchmaschinen unter Umst√§nden kein Problem. Denke daran, dass Google jede Seite zustandslos l√§dt, d.h. sie speichern keine fr√ºheren Informationen und navigieren nicht zwischen den Seiten. Ich habe gesehen, wie SEOs durcheinander gekommen sind, weil sie dachten, es g√§be ein Problem aufgrund der Dinge, die sie sehen, nachdem sie von einer Seite zur anderen navigiert sind, wie z.B. ein kanonisches Tag, das nicht aktualisiert wird. Google sieht jedoch diesen Zustand m√∂glicherweise nie. Entwickler k√∂nnen dies beheben, indem sie den Status mit Hilfe der so genannten History-API aktualisieren, aber auch hier stellt dies m√∂glicherweise kein Problem dar. Aktualisiere die Seite und schau dir an, was du siehst, oder besser noch, lasse sie durch eines von Googles Testtools laufen, um zu pr√ºfen, was Google sieht. Mehr dazu in einer Sekunde.View-source vs. InspectWenn du in einem Browser-Fenster mit der rechten Maustaste klickst, siehst du eine Reihe von Optionen, um den Quellcode der Seite anzuzeigen und die Seite zu inspizieren. View-source wird dir das Gleiche zeigen wie eine GET-Anfrage. Dies ist das rohe HTML der Seite. Inspect zeigt dir das verarbeitete DOM, nachdem √Ñnderungen vorgenommen wurden, und ist n√§her am Inhalt, den Googlebot sieht. Es handelt sich im Grunde um die aktualisierte und neueste Version der Seite. Du solltest eher Inspect statt View-Source verwenden, wenn du mit JavaScript arbeitest.Google CacheGoogles Cache ist kein verl√§√üliches Mittel, um zu √ºberpr√ºfen, was Googlebot sieht. Normalerweise ist es das urspr√ºngliche HTML, allerdings ist es manchmal auch das gerenderte HTML oder eine √§ltere Version. Das System wurde entwickelt, um den Inhalt zu sehen, wenn eine Website nicht verf√ºgbar ist. Als Debugging-Tool ist es nicht besonders n√ºtzlich.Google Testing ToolsGoogles Test-Tools wie der URL-Inspektor in der Google-Suchkonsole, Mobile Friendly Tester, Rich Results Tester sind f√ºr die Fehlersuche n√ºtzlich. Dennoch unterscheiden sich auch diese Tools leicht von dem, was Google sehen wird. Ich habe bereits √ºber das F√ºnf-Sekunden-Timeout in diesen Tools gesprochen, das der Renderer nicht hat, aber diese Tools unterscheiden sich auch dadurch, dass sie Ressourcen in Echtzeit abrufen und nicht die zwischengespeicherten Versionen verwenden, wie es der Renderer tun w√ºrde. Die Screenshots in diesen Tools zeigen auch Seiten mit den dargestellten Pixeln, die Google im Renderer nicht sieht.Die Tools sind jedoch n√ºtzlich, um zu sehen, ob der Inhalt im DOM geladen ist. Das in diesen Tools angezeigte HTML ist das gerenderte DOM. Du kannst nach einem Textausschnitt suchen, um zu sehen, ob er standardm√§√üig geladen wurde.Die Tools zeigen dir auch Ressourcen an, die m√∂glicherweise blockiert sind, und geben Fehlermeldungen in der Konsole aus, die f√ºr die Fehlersuche n√ºtzlich sind.Text in Google suchenEine weitere schnelle √úberpr√ºfung, die du durchf√ºhren kannst, ist die einfache Suche nach einem Ausschnitt deines Inhalts in Google. Suche nach ‚Äûirgendeinem Ausschnitt aus Ihrem Inhalt‚Äù und schaue nach, ob die Seite zur√ºckgegeben wird. Wenn ja, dann wurde dein Inhalt wahrscheinlich gesehen. Beachte, dass Inhalte, die standardm√§√üig ausgeblendet sind, in deinem Snippet in den SERPs m√∂glicherweise nicht angezeigt werden.AhrefsNeben den Seiten mit Linkindex-Rendering kannst du JavaScript in Site Audit-Crawls aktivieren, um mehr Daten in deinen Audits freizuschalten.Die Ahrefs Toolbar unterst√ºtzt auch JavaScript und erm√∂glicht es dir, HTML mit gerenderten Versionen von Tags zu vergleichen.Render OptionenEs gibt viele Optionen, wenn es um die Darstellung von JavaScript geht. Google hat ein solides Diagramm, das ich lediglich zeigen werde. Jede Art von SSR, statisches Rendering, Prerendering-Setup ist f√ºr Suchmaschinen in Ordnung. Das Hauptproblem ist das vollst√§ndige Client-seitige Rendering, bei dem das gesamte Rendering im Browser stattfindet.W√§hrend Google wahrscheinlich sogar mit der clientseitigen Darstellung zufrieden w√§re, ist es am besten, eine andere Darstellungsoption zu w√§hlen, um auch andere Suchmaschinen zu unterst√ºtzen. Bing bietet auch Unterst√ºtzung f√ºr JavaScript-Rendering, aber der Umfang ist unbekannt. Yandex und Baidu haben nach dem, was ich bisher gesehen habe, nur begrenzte Unterst√ºtzung, und viele andere Suchmaschinen haben wenig bis gar keine Unterst√ºtzung f√ºr JavaScript.Es gibt auch die Option des Dynamic Rendering, welches das Rendering f√ºr bestimmte Benutzer-Agenten ist. Dies ist im Grunde ein Workaround, kann aber n√ºtzlich sein, um f√ºr bestimmte Bots wie Suchmaschinen oder sogar Social-Media-Bots zu rendern. Social-Media-Bots f√ºhren kein JavaScript aus, daher werden Dinge wie OG-Tags nur dann angezeigt, wenn Sie den Inhalt rendern, bevor Sie ihn ihnen bereitstellen.Falls du das alte AJAX-Crawling-Schema verwendet hast, beachte, dass dieses veraltet ist und m√∂glicherweise nicht mehr unterst√ºtzt wird.Deine JavaScript-Website SEO-freundlich gestaltenViele der Prozesse √§hneln den Dingen, die SEOs bereits gewohnt sind, aber es k√∂nnte leichte Unterschiede geben.On-Page SEOAlle normalen On-Page-SEO-Regeln f√ºr Inhalte, Titel-Tags, Meta-Beschreibungen, Alt-Attribute, Meta-Robots-Tags usw. gelten weiterhin. Siehe On-Page-SEO: Ein praktikabler Leitfaden.Ein paar Probleme, die ich bei der Arbeit mit JavaScript-Websites immer wieder sehe, sind, dass Titel und Beschreibungen wiederverwendet werden und selten Alt-Attribute auf Bildern gesetzt werden.Crawling zulassenBlockiere nicht den Zugang zu Ressourcen. Google muss in der Lage sein, auf Ressourcen zuzugreifen und diese herunterzuladen, damit die Seiten korrekt dargestellt werden k√∂nnen. In deiner robots.txt, ist der einfachste Weg zu erlauben, die ben√∂tigten Ressourcen zu crawlen, in dem zu folgendes hinzuf√ºgst:User-Agent: Googlebot Allow: .js Allow: .css URLsURLs bei der Aktualisierung von Inhalten √§ndern. Ich habe bereits die History-API erw√§hnt, aber du solltest wissen, dass die JavaScript-Frameworks einen Router haben, mit dem du auf saubere URLs mappen kannst. Du m√∂chten keine Hashes (#) f√ºr das Routing verwenden. Dies ist insbesondere ein Problem f√ºr Vue und einige der fr√ºheren Versionen von Angular. Bei einer URL wie abc.com/#something wird also alles nach einem # in der Regel von einem Server ignoriert. Um dieses Problem f√ºr Vue zu beheben, kannst du mit deinem Entwickler zusammenarbeiten, um Folgendes zu √§ndern:Vue router: Use ‚ÄòHistory‚Äô Mode instead of the traditional ‚ÄòHash‚Äô Mode. const router = new VueRouter ({ mode: ‚Äòhistory‚Äô, router: [] //the array of router links )} Doppelter InhaltMit JavaScript kann es mehrere URLs f√ºr den gleichen Inhalt geben, was zu Problemen mit doppelten Inhalten f√ºhrt. Dies kann durch Gro√üschreibung, IDs, Parameter mit IDs usw. verursacht werden. Diese URLs k√∂nnen also alle existieren:domain.com/Abc domain.com/abc domain.com/123 domain.com/?id=123Die L√∂sung ist einfach. W√§hle eine Version, die indiziert werden soll, und setze kanonische Tags.SEO-‚ÄûPlugin‚Äú-OptionenBei JavaScript-Frameworks werden diese √ºblicherweise als Module bezeichnet. Du findest Versionen f√ºr viele der popul√§ren Frameworks wie React, Vue und Angular, indem du nach dem Framework + Modulnamen wie ‚ÄúReact Helmet‚Äù suchst. Meta-Tags, Helm und Kopf sind allesamt beliebte Module mit √§hnlicher Funktionalit√§t, die es die M√∂glichkeit bieten, viele der beliebten Tags zu setzen, die f√ºr SEO ben√∂tigt werden.FehlerseitenDa JavaScript-Frameworks nicht server-seitig sind, k√∂nnen sie nicht wirklich einen Serverfehler wie einen 404 ausl√∂sen. Du hast eine Reihe verschiedener Optionen Fehlerseiten zu erstellen:Verwende eine JavaScript-Umleitung zu einer Seite, die mit einem 404-Statuscode antwortetF√ºge einen noindex-Tag zu der Seite hinzu, die fehlgeschlagen ist, zusammen mit einer Art Fehlermeldung wie ‚Äû404 Page Not Found‚Äú. Dies wird wie eine weiche 404 behandelt, da der tats√§chlich zur√ºckgegebene Statuscode ein Okay mit 200 ist.SitemapJavaScript-Frameworks haben in der Regel Router, die auf saubere URLs abbilden. Diese Router verf√ºgen in der Regel √ºber ein Zusatzmodul, das auch Sitemaps erstellen kann. Du kannst sie finden, indem du nach deinem System + Router-Sitemap suchst, z.B. ‚ÄûVue router sitemap‚Äú. Viele der Rendering-L√∂sungen verf√ºgen m√∂glicherweise auch √ºber Sitemap-Optionen. Auch hier gilt: Finde einfach das von dir verwendete System und google nach dem System + Sitemap, z. B. ‚ÄûGatsby-Sitemap‚Äú, und du wirst sicher eine L√∂sung finden, die bereits existiert.RedirectsSEOs sind an 301/302-Weiterleitungen gew√∂hnt, welche serverseitig sind. Javascript wird jedoch typischerweise clientseitig ausgef√ºhrt. Dies ist in Ordnung, da Google die Seite wie nach der Weiterleitung verarbeitet. Die Weiterleitungen geben weiterhin alle Signale wie PageRank weiter. Du kannst diese Weiterleitungen normalerweise im Code finden, indem du nach ‚Äúwindow.location.href‚Äù suchst.InternationalisierungEs gibt in der Regel einige wenige Moduloptionen f√ºr verschiedene Frameworks, die einige f√ºr die Internationalisierung erforderliche Funktionen wie hreflang unterst√ºtzen. Sie wurden auf die verschiedenen Systeme portiert und umfassen i18n, intl. In vielen F√§llen k√∂nnen dieselben Module, die f√ºr Header-Tags wie Helmet verwendet werden, verwendet werden, um ben√∂tigte Tags hinzuzuf√ºgen.Lazy LoadingEs gibt in der Regel Module f√ºr die Handhabung von Lazy Loading. Falls du es noch nicht bemerkt hast, es gibt Module f√ºr so ziemlich alles, was du bei der Arbeit mit JavaScript-Frameworks tun musst. Lazy und Suspense sind die beliebtesten Module f√ºr Lazy Loading. Du wirst Bilder ‚Äûlazy‚Äú laden wollen, aber sei vorsichtig, dass du keine Inhalte ‚Äûlazy‚Äú l√§dst. Dies kann mit JavaScript erreicht werden, aber es k√∂nnte bedeuten, dass der Inhalt von Suchmaschinen nicht korrekt erfasst wird.Abschlie√üende GedankenJavaScript ist ein Werkzeug, das mit Bedacht eingesetzt werden sollte, nicht aber etwas, wovor sich SEOs sich f√ºrchten m√ºssen. Hoffentlich hat dir dieser Artikel geholfen, besser zu verstehen, wie du damit besser umgehen kannst, aber scheue dich nicht, dich an deine Entwickler zu wenden und mit ihnen zusammenzuarbeiten und ihnen Fragen zu stellen. Sie werden deine gr√∂√üten Verb√ºndeten sein, wenn es darum geht, deine JavaScript-Website f√ºr Suchmaschinen zu verbessern.Hast du Fragen? Lasse es mich auf Twitter wissen.Weitere InformationenJavascript SEO BasicsJavaScript SEO-B√ºrozeitenJavaScript-Sites ArbeitsgruppeJavaScript SEO Video Reihe√úber¬≠set¬≠zt von sehrausch.de: Such¬≠¬≠maschi¬≠¬≠nen‚Äì & Con¬≠ver¬≠¬≠¬≠¬≠sion-Opti¬≠mierung, Online-Mar¬≠ket¬≠ing & Paid-Adver¬≠tis¬≠ing. Pass¬≠ge¬≠nau aus ein¬≠er Hand. Article PerformanceData from AhrefsVerlinkung von Websites 3 Sign up for AhrefsGet SEO metrics of any website or URL.The number of websites linking to this post.This post's estimated monthly organic search traffic.Get the week's best marketing content Email Subscription Subscribe Leave this field empty if you're human: Artikel von Patrick Stox Patrick Stox ist Produktberater, technischer SEO und Markenbotschafter bei Ahrefs. Er ist Organisator des SEO-Meetups in Raleigh, der SEO-Konferenz in Raleigh, des Beer & SEO-Meetups, der Findability-Konferenz und Moderator bei /r/TechSEO.",
  "headers": [
    {
      "level": "H1",
      "text": "JavaScript SEO: Was du wissen musst"
    },
    {
      "level": "H2",
      "text": "Was ist Javascript SEO?"
    },
    {
      "level": "H2",
      "text": "Wie Google Seiten mit JavaScript verarbeitet"
    },
    {
      "level": "H3",
      "text": "1. Crawler"
    },
    {
      "level": "H3",
      "text": "2. Verarbeitung"
    },
    {
      "level": "H4",
      "text": "Ressourcen und¬†Links"
    },
    {
      "level": "H4",
      "text": "Caching"
    },
    {
      "level": "H4",
      "text": "Beseitigung von Duplikaten"
    },
    {
      "level": "H4",
      "text": "Die restriktivsten Richtlinien"
    },
    {
      "level": "H3",
      "text": "3. Render Liste"
    },
    {
      "level": "H3",
      "text": "4. Renderer"
    },
    {
      "level": "H4",
      "text": "Gecachte Resourcen"
    },
    {
      "level": "H4",
      "text": "Kein festes Timeout"
    },
    {
      "level": "H4",
      "text": "Was Googlebot sieht"
    },
    {
      "level": "H3",
      "text": "5. Crawl¬†Liste"
    },
    {
      "level": "H2",
      "text": "Test/Fehlerbehebung"
    },
    {
      "level": "H3",
      "text": "View-source vs. Inspect"
    },
    {
      "level": "H3",
      "text": "Google Cache"
    },
    {
      "level": "H3",
      "text": "Google Testing Tools"
    },
    {
      "level": "H3",
      "text": "Text in Google suchen"
    },
    {
      "level": "H3",
      "text": "Ahrefs"
    },
    {
      "level": "H2",
      "text": "Render Optionen"
    },
    {
      "level": "H2",
      "text": "Deine JavaScript-Website SEO-freundlich gestalten"
    },
    {
      "level": "H3",
      "text": "On-Page SEO"
    },
    {
      "level": "H3",
      "text": "Crawling zulassen"
    },
    {
      "level": "H3",
      "text": "URLs"
    },
    {
      "level": "H3",
      "text": "Doppelter Inhalt"
    },
    {
      "level": "H3",
      "text": "SEO-‚ÄûPlugin‚Äú-Optionen"
    },
    {
      "level": "H3",
      "text": "Fehlerseiten"
    },
    {
      "level": "H3",
      "text": "Sitemap"
    },
    {
      "level": "H3",
      "text": "Redirects"
    },
    {
      "level": "H3",
      "text": "Internationalisierung"
    },
    {
      "level": "H3",
      "text": "Lazy Loading"
    },
    {
      "level": "H2",
      "text": "Abschlie√üende Gedanken"
    }
  ],
  "author": "Patrick Stox"
}