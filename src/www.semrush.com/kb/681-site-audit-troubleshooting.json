{
  "url": "https://www.semrush.com/kb/681-site-audit-troubleshooting",
  "title": "Troubleshooting Site Audit",
  "content": "Troubleshooting Site Audit\nTry Site Audit\n\nIs your Site Audit not running properly?\n\nThere are a number of reasons why pages could be blocked from the Site Audit crawler based on your website’s configuration and structure, including:  \n\nRobots.txt blocking crawler\nCrawl scope excluding certain areas of the site\nWebsite is not directly online due to shared hosting\nLanding page size exceeding 2Mb\nPages are behind a gateway / user base area of site\nCrawler blocked by noindex tag\nDomain could not be resolved by DNS — the domain entered in setup is offline\nWebsite content built on JavaScript — while Site Audit can render JS code, it can still be the reason for some of the issues\nTroubleshooting Steps\n\nFollow these troubleshooting steps to see if you can make any adjustments on your own before reaching out to our support team for help.\n\nCheck your Robots.txt for Disallow Commands\nRemove Restrictive Tags from your Site\nWhitelist SemrushBot\nCheck your page size\nCheck Account Limits\nProper Redirects (for DNS Issues)\nJavaScript-related issues\nChange the User Agent\nBypass Disallow in Robots.txt\nCrawl with your Credentials\nChanging Crawler Settings\n\nCheck out our post about the Biggest SEO Mistakes Damaging Websites.",
  "headers": {
    "h1": "\n    Troubleshooting Site Audit\n  ",
    "h2": []
  },
  "images": [
    {
      "src": "https://cdn.semrush.com/kb/static/kb/img/icons/menu.cfe7dbdefa43.svg",
      "alt": "menu button"
    },
    {
      "src": "https://cdn.semrush.com/kb/static/img/icons/cross.fb7587565274.svg",
      "alt": "close button"
    },
    {
      "src": "https://img.youtube.com/vi/D5qewEsHdDY/maxresdefault.jpg",
      "alt": "Troubleshooting Site Audit image 1"
    },
    {
      "src": "https://static.semrush.com/kb/uploads/2023/10/06/robots%20txt%20example.png",
      "alt": "List of allow and disallow directives in a robots.txt example file. Disallow directives are highlighted in red, allow directives - in light green. There are also additional instructions on the right side of the screenshot: Disallow = instruction for bots to NOT crawl this area of the site (in red), Allow = instruction for bots to crawl this area of the site (in green)."
    },
    {
      "src": "https://static.semrush.com/kb/uploads/2023/10/15/SA%20JS.png",
      "alt": "A demonstration on where to enable JavaScript rendering in Site Audit settings. Highlights show the correct tab and settings section."
    },
    {
      "src": "https://static.semrush.com/kb/uploads/2023/10/15/SA%20sitemap%20source.png",
      "alt": "Demonstration on where to find the Crawl Source settings in Site Audit. The dropdown menu is highlighted and features all available Crawl source options."
    },
    {
      "src": "https://static.semrush.com/kb/uploads/2024/07/29/for%20681.png",
      "alt": "Instruction on where to find the user agent settings in Site Audit. In Overview report, the gear icon on the top right is highlighted to indicate the dropdown menu that opens when user clicks on it. The settings menu can be scrolled down to find the exact settings needed, user agent setting in this case (this line is highlighted in the menu as well)"
    },
    {
      "src": "https://static.semrush.com/kb/uploads/2024/07/29/credentialscrawling.gif",
      "alt": "Instruction on how to enable 'crawling with credentials' option in Site Audit. In this GIF, cursor opens the settings via the gear icon on the top right and selects 'Crawling with credentials' option after scrolling down a little. After that, a pop-up menu appears for Bypassing website restrictions. After the second box is checked on that tab, test credentials are entered and cursor goes down to save settings."
    },
    {
      "src": "https://cdn.semrush.com/kb/static/img/icons/arrow-left.aa31ace2c168.svg",
      "alt": "Back to"
    }
  ],
  "publication_date": null
}