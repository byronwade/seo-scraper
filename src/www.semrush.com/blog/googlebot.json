{
  "url": "https://www.semrush.com/blog/googlebot/",
  "title": "What Is Googlebot? How Google‘s Web Crawler Works",
  "content": "What Is Googlebot?\n\nGooglebot is the main program Google uses to automatically crawl (or visit) webpages. And discover what's on them. \n\nAs Google’s main website crawler, its purpose is to keep Google’s vast database of content, known as the index, up to date. \n\nBecause the more current and comprehensive this index is, the better and more relevant your search results will be.\n\nThere are two main versions of Googlebot:\n\nGooglebot Smartphone: The primary Googlebot web crawler. It crawls websites as if it were a user on a mobile device. \nGooglebot Desktop: This version of Googlebotcrawls websites as if it were a user on a desktop computer. Checking the desktop version of your site.\n\nThere are also more specific crawlers like Googlebot Image, Googlebot Video, and Googlebot News.\n\nWhy Is Googlebot Important for SEO?\n\nGooglebot is crucial for Google SEO because your pages wouldn’t be crawled and indexed (in most cases) without it. If your pages aren’t indexed, they can’t be ranked and shown in search engine results pages (SERPs). \n\nAnd no rankings means no organic (unpaid) search traffic.\n\nPlus, Googlebot regularly revisits websites to check for updates. \n\nWithout it, new content or changes to existing pages wouldn't be reflected in search results. And not keeping your site up to date can make maintaining your visibility in search results more difficult.\n\nHow Googlebot Works\n\nGooglebot helps Google serve relevant and accurate results in the SERPs by crawling webpages and sending the data to be indexed.\n\nLet’s look at the crawling and indexing stages more closely:\n\nCrawling Webpages\n\nCrawling is the process of discovering and exploring websites to gather information. Gary Illyes, an analyst at Google, explains the process in this video:\n\nGooglebot is constantly crawling the internet to discover new and updated content.\n\nIt maintains a continuously updated list of webpages. Including those discovered during previous crawls along with new sites.\n\nThis list is like Googlebot’s personal adventure map. Guiding it on where to explore next.\n\nBecause Googlebot also follows links between pages to continuously discover new or updated content. \n\nLike this:\n\nOnce Googlebot discovers a page, it may visit and fetch (or download) its content. \n\nGoogle can then render (or visually process) the page. Simulating how a real user would see and experience it.\n\nDuring the rendering phase, Google runs any JavaScript it finds. JavaScript is code that lets you add interactive and responsive elements to webpages.\n\nRendering JavaScript lets Googlebot see content in a similar way to how your users see it.\n\nNote\n\nGooglebot doesn’t crawl every page it finds. For example, pages that aren’t publicly accessible. Or ones that don’t meet a certain quality threshold. Use Semrush’s Site Audit tool to identify any crawlability issues that could be preventing Googlebot from accessing your content. \n\nOpen the tool, insert your domain, and click “Start Audit.”\n\nIf you’ve already run an audit or created projects, click the “+ Create project” button to set up a new one.\n\nEnter your domain, name your project, and click “Create project.”\n\nNext, you’ll be asked to configure your settings. \n\nIf you’re just starting out, you can use the default settings in the “Domain and limit of pages” section.\n\nThen, click on the “Crawler settings” tab to pick the user agent you would like to crawl with. A user agent is a label that tells websites who's visiting them. Like a name tag for a search engine bot.\n\nThere is no major difference between the bots you can choose from. They’re all designed to crawl your site like Googlebot would.\n\nCheck out our Site Audit configuration guide for more details on how to customize your audit.\n\nWhen you’re ready, click “Start Site Audit.”\n\nYou’ll then see an overview page like below. Navigate to the “Issues” tab. \n\nHere, you’ll see a full list of errors, warnings, and notices affecting your website’s health. \n\nClick the “Category” drop-down and select “Crawlability” to filter the errors.\n\nNot sure what an error means and how to address it? \n\nClick “Why and how to fix it” or “Learn more” next to any row for a short explanation of the issue and tips on how to resolve it.\n\nGo through and fix each issue to make it easier for Googlebot to crawl your website.\n\nFind and Fix Crawlability Issues\n\nwith the Site Audit Tool\n\nSign Up Now →\nIndexing Content\n\nAfter GoogleBot crawls your content, it sends it for indexing consideration. \n\nIndexing is the process of analyzing a page to understand its contents. And assessing signals like relevance and quality to decide if it should be added to Google’s index.\n\nHere’s how Google’s Gary Illyes explains the concept: \n\nDuring this process, Google processes (or examines) a page’s content. And tries to determine if a page is a duplicate of another page on the internet. So it can choose which version to show in its search results.\n\nOnce Google filters out duplicates and assesses relevant signals, like content quality, it may decide to index your page. \n\nThen, Google’s algorithms perform the ranking stage of the process. To determine if and where your content should appear in search results.\n\nSide note\n\nThere’s no guarantee that Google will index your page. For example, low-quality pages and those with a poor user experience may not be indexed. Use Semrush’s Site Audit to discover any issues that could be keeping your site from showing up in search results. \n\nFrom your “Issues” tab, filter for “Indexability.” Make your way through the errors first. Either by yourself or with the help of a developer. Then, tackle the warnings and notices.\n\nFurther reading: Crawlability & Indexability: What They Are & How They Affect SEO\n\nHow to Monitor Googlebot's Activity\n\nRegularly checking Googlebot’s activity lets you spot any indexability and crawlability issues. And fix them before your site’s organic visibility falls. \n\nHere are two ways to do this:\n\nUse Google Search Console’s Crawl Stats Report\n\nUse Google Search Console’s “Crawl stats” report for an overview of your site’s crawl activity. Including information on crawl errors and average server response time.\n\nTo access your report, log in to Google Search Console property and navigate to “Settings” from the left-hand menu. \n\nScroll down to the “Crawling” section. Then, click the “Open Report” button in the “Crawl stats” row.\n\nYou’ll see three crawling trends charts. Like this:\n\nThese charts show the development of three metrics over time:\n\nTotal crawl requests: The number of crawl requests Google’s crawlers (like Googlebot) have made in the past three months\nTotal download size: The number of bytes Google crawlers have downloaded while crawling your site\nAverage response time: The amount of time it takes for your server to respond to a crawl request\n\nTake note of significant drops, spikes, and trends in each of these charts. And work with your developer to spot and address any issues. Like server errors or changes to your site structure.\n\nThe “Crawl requests breakdown” section groups crawl data by response, file type, purpose, and Googlebot type.\n\nHere’s what this data tells you:\n\nBy response: Shows you how your server has handled Googlebot’s requests. A high percentage of “OK (200)” responses are a good sign. It means most pages are accessible. On the other hand, errors like 404 or 301 can indicate broken links or moved content that you may need to fix.\nBy file type: Tells you the type of files Googlebot is crawling. This can help uncover issues related to specific file types, like images or JavaScript.\nBy purpose: Indicates the reason for a crawl. A high discovery percentage indicates Google is dedicating resources to finding new pages. High refresh numbers mean Google is frequently checking existing pages.\nBy Googlebot type: Shows which Googlebot user agents are crawling your site. If you’re noticing crawling spikes, your developer can check the user agent type to determine whether there is an issue.\nAnalyze Your Log Files\n\nLog files are documents that record details about every request made to your server by browsers, people, and other bots. Along with how they interact with your site. \n\nBy reviewing your log files, you can find information like: \n\nIP addresses of visitors\nTimestamps of each request\nRequested URLs\nThe type of request\nThe amount of data transferred \nThe user agent, or crawler bot\n\nHere’s what a log file looks like:\n\nAnalyzing your log files lets you dig deeper into Googlebot’s activity. And identify details like crawling issues, how often Google crawls your site, and how fast your site loads for Google.\n\nLog files are kept on your web server. So to download and analyze them, you first need to access your server.\n\nSome hosting platforms have built-in file managers. This is where you can find, edit, delete, and add website files.\n\nAlternatively, your developer or IT specialist can also download your log files using a File Transfer Protocol (FTP) client like FileZilla. \n\nTip\n\nCheck out our guide to log file analysis for more information on log files and a step-by-step breakdown of how to conduct an analysis.\n\nOnce you have your log file, use Semrush’s Log File Analyzer to understand that data. And answer questions like:\n\nWhat are your most crawled pages?\nWhat pages weren’t crawled?\nWhat errors were found during the crawl?\n\nOpen the tool and drag and drop your log file into it. Then, click “Start Log File Analyzer.”\n\nOnce your results are ready, you’ll see a chart showing Googlebot’s activity on your site in the past 30 days. This helps you identify unusual spikes or drops.\n\nYou’ll also see a breakdown of different status codes and requested file types.\n\nScroll down to the “Hits by Pages” table for more specific insights on individual pages and folders. \n\nYou can use this information to look for patterns in response codes. And investigate any availability issues. \n\nFor example, a sudden increase in error codes (like 404 or 500) across multiple pages could indicate server problems causing widespread website outages.\n\nThen, you can contact your website hosting provider to help diagnose the problem and get your website back on track.\n\nAnalyze Your Website's Log Files\n\nwith Log File Analyzer\n\nSign Up Now →\nHow to Block Googlebot \n\nSometimes, you might want to prevent Googlebot from crawling and indexing entire sections of your site. Or even specific pages. \n\nThis could be because:\n\nYour site is under maintenance and you don’t want visitors to see incomplete or broken pages\nYou want to hide resources like PDFs or videos from being indexed and appearing in search results\nYou want to keep certain pages from being made public, like intranet or login pages\nYou need to optimize your crawl budget and ensure Googlebot focuses on your most important pages\n\nHere are three ways to do that:\n\nRobots.txt File\n\nA robots.txt file is a set of instructions that tells search engine crawlers, like Googlebot, which pages or sections of your site they should and shouldn’t crawl. \n\nIt helps manage crawler traffic and can prevent your site from being overloaded with requests.\n\nHere’s an example of a robots.txt file:\n\nFor example, you could add a robots.txt rule to prevent crawlers from accessing your login page. This helps keep your server resources focused on more important areas of your site.\n\nLike this:\n\nUser-agent: Googlebot\nDisallow: /login/\n\nFurther reading: Robots.txt: What Is Robots.txt & Why It Matters for SEO\n\nHowever, robots.txt files don’t necessarily keep your pages out of Google’s index. Because Googlebot can still find these pages (e.g., if other pages link to them), and then they may still be indexed and shown in search results. \n\nIf you don’t want a page to appear in the SERPs, use meta robots tags.\n\nMeta Robots Tags\n\nA meta robots tag is a piece of HTML code that lets you control how an individual page is crawled, indexed, and displayed in the SERPs.\n\nSome examples of robots tags, and their instructions, include:\n\nnoindex: Do not index this page\nnoimageindex: Do not index images on this page\nnofollow: Do not follow the links on this page\nnosnippet: Do not show a snippet or description of this page in search results\n\nYou can add these tags to the <head> section of your page’s code. For example, if you want to block Googlebot from indexing your page, you could add a noindex tag. \n\nLike this:\n\n<meta name=\"googlebot\" content=\"noindex\">\n\nThis tag will prevent Googlebot from showing the page in search results. Even if other sites link to it.\n\nFurther reading: Meta Robots Tag & X-Robots-Tag Explained\n\nPassword Protection\n\nIf you want to block both Googlebot and users from accessing a page, use password protection. \n\nThis method ensures that only authorized users can view the content. And it prevents the page from being indexed by Google.\n\nExamples of pages you might password protect include:\n\nAdmin dashboards\nPrivate member areas\nInternal company documents\nStaging versions of your site\nConfidential project pages\n\nIf the page you’re password protecting is already indexed, Google will eventually remove it from its search results.\n\nMake It Easy for Googlebot to Crawl Your Website\n\nHalf the battle of SEO is making sure your pages even show up in the SERPs. And the first step is ensuring Googlebot can actually crawl your pages.\n\nRegularly monitoring your site’s crawlability and indexability helps you do that.\n\nAnd finding issues that might be hurting your site is easy with Site Audit. \n\nPlus, it lets you run on-demand crawling and schedule auto re-crawls on a daily or weekly basis. So you’re always on top of your site’s health.\n\nTry it today.\n\nFind and Fix Crawlability Issues\n\nwith the Site Audit Tool\n\nSign Up Now →",
  "headers": {
    "h1": "What Is Googlebot? How Google's Web Crawler Works",
    "h2": [
      "What Is Googlebot?",
      "Why Is Googlebot Important for SEO?",
      "How Googlebot Works",
      "How to Monitor Googlebot's Activity",
      "How to Block Googlebot ",
      "Make It Easy for Googlebot to Crawl Your Website"
    ]
  },
  "images": [
    {
      "src": "https://static.semrush.com/blog/uploads/media/ca/34/ca34f6e943ed7aabb7d858311457a552/ce4676642da4c049a5897fc0faaac77c/googlebot.svg",
      "alt": "Googlebot"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/b6/f6/b6f606974f1893626377db52746df881/03878be9d87d53259fa6c2e0aa1148ee/AD_4nXcplBn3i7jMcHzW2QFPsCDBiophy6OnwIZXnsTU-XobcOOk8HLeiS3smpNb1Lana7EeExXe_sPBi6JbHAUC9l9Ra8fWdbatyb7T4nH2rwTO6dMY8pzymsnsSAd9BbLUQBTxdGSq7MXx3ckAl7xtqEEdPuQ.png",
      "alt": "How Search Engines Work: Google bots crawling your site, indexing your page and then being shown on the SERP if it meets ranking criteria."
    },
    {
      "src": "https://i.ytimg.com/vi/JuK7NnfyEuc/hq720.jpg",
      "alt": "Youtube video thumbnail"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/e0/d9/e0d9892c03c6e70e4797bdc6d991e196/4cd98909c4ab97b1362b5d02956cbe17/AD_4nXf3DBr2t0E1ZL91YCCAuzAUUnJZnNBRPokxn6qHelJaoDey_TFvpv6W1WEddMdBENCSvGibJPSxU1RcfToqzU9UV_0MWtQ2rc1vfvTsEX4_NXL7Yn8GH2kzq59ylrbkEeIJZr7_AJ9rFpGMisSnIYKu-YrU.jpeg",
      "alt": "Googlebot following links between pages to continuously discover new or updated content."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/9b/57/9b57bda1058ca88bee25e8119ae0cea3/ff13cabf1aa6a9c6eae11ad29a8898b0/AD_4nXcdaDHsjK3ZbFB_OxbdBAVBaDBjptTfZ0FkRznzmbvseB8AReAdcaQb8y2wK2JoeIR5HqcMG466uSyCDDep9_7aMJqfoTZrWsij4R92V2a9kCWPaHx3iq9-0s0YpCzmfMgZaJ8kywRj2S0QEhtlUWgs6MGT.jpeg",
      "alt": "Site Audit search with a domain entered and the \"Start Audit\" button clicked."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/85/56/855608684944333caa43786b565c6b8c/540f6f7e763a87c78338311a92040432/AD_4nXfAkO92W3Sx2gFt5ytiS7YcKs9mVe6175F9P1ZS3f_AZ1lD15kowHwtMJXnEswLoc4l3Kk-5J18twA92tX5Y-hK494Mu8TNiU4k68e6n-j5lOY9UsKi9i9nMieD-Zq9EV8vYf5_425y3AC68ePXTudSFc7r.png",
      "alt": "\"Projects\" page on Site Audit with the “+ Create project” button clicked."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/f3/32/f332f02b44bc993073c375b7208ec095/15db833c48efb0689ffb602475a0fa12/AD_4nXf3hGjsTdcdIGXoYPxNteaQKwfJIQioNiEDMnq2kSdqURrOMjLY8EBIU2k48F6bYKxkyeVm_jI8j37hHPMyI9im6P8BLk9hPFuueeWB1PyzUeJMoG8V-cwek1xFG5P8MhkR2TR0uKCDh6l4o3GDpBvSKGsf.jpeg",
      "alt": "Input boxes to enter a domain and project name along with the \"Create project\" button clicked."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/6d/1b/6d1b55de8fb00eb32207124ce6f9f5b2/4f0e6837cc7321cd148bb9c3b0eaf62d/AD_4nXcI5GJSVvDSFAX4lWXFrxSYIxb5izyhWu1tz35RbGzN3GuP5fALfugT03mHVARNou4RyRb462W_v4xixDkS2BWOyXgdlHtaGhhn436biVf5bktOpIR02f3p6OqKoe_VGbvBoUfnEqu1mL5sDIhNyTJZaONz.jpeg",
      "alt": "Crawler settings page on Site Audit with the \"User agent\" section highlighted."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/23/cf/23cfc45e4ee2c0c4044cddbf60e6065a/a210685d24d8839c6e3f6746599a63cb/AD_4nXckM3XE97p8zv_alul6dbVDOujUzNnuWSQaXlM59hAbi6IWpaSQZBEic9W4qn7LEriEx_bhD4MRiiQEmDKrSV7xEdXRdaWmw2acrgdYlX43qHf2vziOSX-mzLko-T5ssUoLMIVfDI29aLZMEGXP2fUA_pHB.jpeg",
      "alt": "Scheduling settings page on Site Audit with the \"Start Site Audit\" button clicked."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/a0/6f/a06f8184f684640469f9f2ee105c4e9a/ca956b7e74bdf6e05a16c67e97e7610f/AD_4nXchjvQFcVg6swatU2he7PdFjXTzg5EIHIysrSgtd1dl9_7UYs848Go8lt_Oz0iiSlqFLiOSeIigxZBYOp4pXPzFt1Q9jDKK5JNfqufWx-oCcG48C8IHd5pOdewZ30sE2JXupS9ntJq3Kckfoa7v_BafKtSj.jpeg",
      "alt": "Site Audit overview report with the \"Issues\" tab highlighted."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/a0/05/a0054a91f7acf5524b66d744ac67c497/cd18085688ed32f984f11ddaf937035a/AD_4nXdMdAWY__NN5WZ1Borjzx4HFIns_u_3UBmtsLf_O56LwmryCsp_C9OcXx2o_NlZ90g4CPYtNoGfrj6F6X6SsyiS6uteEKN1ZMWyCcfslRDmshslruRQ0gIvd6eRoj8cKsCh6fyi0IH9S1KquVcRy7F5-9Ee.jpeg",
      "alt": "Site Audit Issues page with the \"Category\" dropdown opened and \"Crawlability\" selected."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/32/37/3237cfab3384b0f0d6a0867050a53f60/cf0168fbf16fa4e43029ff8f836dbda5/AD_4nXda8aUetFxJRwTX3DH3JTZjlJYjUAjNrPBSFk98JMwoXB208h86R1jc5nRkUUxtSw1V7-sAakX-FgA4xe8Ln9GjgVMjNVEtzIy_300QTV6jMw5s1uNJpQOuZo2qHQR0bfgZcTBJaJFYYtsr4aEVIV99k8jk.jpeg",
      "alt": "Crawlability issues with “Why and how to fix it” next to broken internal link issues clicked, showing tips on how to resolve the issue."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/files/e3/c1/e3c11aed0d9bffb525e8f7552d6b7fa1/illustration-ads-banner-205x170.svg",
      "alt": "ADS illustration"
    },
    {
      "src": "https://i.ytimg.com/vi/pe-NSvBTg2o/hq720.jpg",
      "alt": "Youtube video thumbnail"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/aa/70/aa7085c488f808e894b72d5f593a6b7b/bd5c71f593dc201e1428e2ed90281963/AD_4nXfzLq0BbSwOatwZ61VCSpw2k29SuQxZCO_Ay6cX98-aLSqGA0EtxXerA3zQXy2N-olJSagcUj6JY1zbjg2YKmh5aIba6iIZN2mJlygONjWWIBkBAdROr1HF_xQNwQKsSfHEU_t9ONQSmJQQSvuET-MtS2bD.jpeg",
      "alt": "Indexability issues on Site Audit like hreflang conflicts within page source code, duplicate content issues, etc."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/de/d8/ded82c3a8b21669e0ea7ab6563efff74/e38ed7ce6ea888f3fd2bf451915fbb16/AD_4nXdsiuF7JyCuFJkSyUfeZzab_J-SiFx5GfJDAMtA4GDGdMr7MRGyIkPlYZ4PGTw1PubtjqQ6vd761H6rpeIlU4jXEkhKa9fPluzWccqXB-G_JwKsAlennd6Y2DPPDdytABZHOjawI3VL6gphRCBKVq8Wteb1.jpeg",
      "alt": "Left-hand side navigation bar on Google Search Console with \"Settings\" clicked."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/95/b5/95b57f885bf5398bae55b3af9e589d83/ab3eb226587ed4e1c5dea0a98e3fbd9f/AD_4nXdc9MVt3mlja902mJqZTTI1WYl46WFQyaa8I6KBJjZEJLCZj96ax7nkSeH9lREgdxxjh677IlGMjTLf_aXMw3IpTAqBhhqXTpncqOxt3yKm9wwZiMdR3a-OKA8tjJYX47Ca_eWdUJlAHC3cpOp38e1-nqZg.jpeg",
      "alt": "Settings page on Google Search Console with \"Crawling\" highlighted and \"Open Report\" next to \"Crawl stats\" clicked."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/8f/8b/8f8be3f7b2943f97bf992122403217ca/d2a55ae6733cd3ee1a4a9ffc3b00d03f/AD_4nXfjEn0AtjepD5IWafD8ZnGwZpjL_N6x-rBEDov1kEXz1u5q-77SKWHrbOovC7aAdPLBYFn4b16MXKXt77qH26eBGGo9e6YJcofQG2KLDs8juNy9Epkmf8Odq2fDnbl5uBUk16UqDMZotpXX8OhmTXpoNj0Q.jpeg",
      "alt": "Crawl stats chart showing graphs over time for \"Total crawl requests\", \"Total download size\", and \"Average response time\"."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/64/6a/646a9fd854cb279a93368241c1cd6e1a/25b814b73e00957a378e711b50627ecb/AD_4nXeFU7opLGnri6Y9FBNLzGdCUwFJjDsSXs3GVE1NcNshp_N5MgQ4E_sPOqheyorWx0cPvflDr6uvTZxTgS9gCfL1b4Gj74zCGezXFW1VQvP4XrX-SZG9tZidBRaGl0FTT7nKyxw-Mc-BcpKcd64q-AyPqDxx.jpeg",
      "alt": "Crawl requests breakdown showing crawl data grouped by response, file type, purpose, and Googlebot type."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/30/a5/30a55256da43f3d204b5f622a3798ba2/610b8038c2152a8055783e4f6ce7eff1/AD_4nXe9hdSrzAyxzTpn_nJGPpwR-9FzQvnz_KmUvBwflqy7Msgyg7IeH4RlQEvIh4IccR88pAJhsGhQwddoh1hSVmVJlCF4FDzB3q8xGRakPIBIRczhXkbM2dmlXMGg57NWznTwJUZOchJmoyvDUYmogYAnbagO.jpeg",
      "alt": "Example of a log file that with information about different requests made to a server."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/08/c8/08c8c05f58015d1f75b5dd2be48d902b/bda7e759a22b91d5cf75f3f424abe211/AD_4nXfyv8Z3ni5CJmhJwJDlHkCPRnNib0X1p_eMlSyFg5a2Kb2f_Vjir1z06zVSw5slOnY8E5vmdSxLpRBSc7Zg1UKJTvicwWvHEcG14UBrLAFNBiOzNINjZZlqfeeWzUffQVJF-Q-bgKssFFO88qCV_pRCfT0.jpeg",
      "alt": "A built-in file manager on a hosting platform dashboard to find, edit, delete, and add website files."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/de/da/dedaecaa1830016e4df784cd8ceb514a/33611ab381c36a7f646d441b007c1aa4/AD_4nXcDHo2Sr-8DLK6yQOlIrZ1eG3mvUC8bHHUU4RU-lh7PE7ImRYMyGR_Gyt4zqtMwPE079M1OW_tKhSw04fS6SSzjXpUe0kUQgZLmmgeGgrvqvp2wSbGlALVxkVgMPanK6XTwLUSLr9YHT80ePQpAHSbHxmo.png",
      "alt": "Log File Analyzer tool start with a section to drag & drop or browse for log files."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/81/15/81155a1117f159d9a273e7473fd6f4d8/3d69e2ca38930bd63a64ecd19467b492/AD_4nXcqPfUpnF4iQH9Pag_5dvNwY1yx-qTdl-VkoJrZzBZ1hO98uZLgskHi3DfgG9QqzzGEUDSTbcoGqPR6wz25wVQtHSmH_Acm1WkNk5KgmKrYrOjJNZcmwtBCemKu35nZivoZa1jCLlpNXkR345MvM-L4cP8.jpeg",
      "alt": "Googlebot’s activity on a site along with a breakdown of different status codes and requested file types."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/95/80/95804f062737cc59c132fa4a27235cf2/44b3ff6066fd62782174f1d3e6668e98/AD_4nXd6j8XCqy238NLdeJKtOBLWnMturJSYAI-Ei91CShBku-oqvdrT7nRtWZDdS560jJ9cke7MrG8vgnTvH64bRbSC8RGBg2P7WJQATIk8YbMOIiF5XTczzTvsmm1ljS0RWYoJsdtpPXrjsZ0E2MvkF1RoQRkD.jpeg",
      "alt": "“Hits by Pages” table on Log File Analyzer with specific data and insights for individual pages and folders."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/files/e3/c1/e3c11aed0d9bffb525e8f7552d6b7fa1/illustration-ads-banner-205x170.svg",
      "alt": "ADS illustration"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/a0/9a/a09aa657cdcfb62f9fde6431d2cdfcac/4f9dbbc86126d2bee2d739bfea6b948c/AD_4nXdPGrUElq6kN3deEp5K6b7jgMzSr88rHeYIBwF6i03w0_CAFafAOC1r6QpqampRyHDofmegWhEoWb6mRVqgtFuhu5Yut2Ne8A3akzqrAvnwwreAfa50n9LZyZCfflgG2J3_FhzLbsaxk5Mc23ywVGF7NII.jpeg",
      "alt": "Example of a robots.txt file showing pages or sections of a site that should and shouldn’t be crawled."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/8b/a6/8ba63686494e3d3e5bc46fa32185a2f5/ab5a2451e4bf3a6c863a6865131e225a/AD_4nXdM-u3Mq2vScfyx3EnxorF4oCBo__85-5tm8x6n8Pljxzp4DL6zAmPmQrccUgW1X91O9-hddtcnTkahGb8feY8NVGR-hRY4CfaZz12YKbhQSiRYxpvEwI_NlP6ftMdK_gIyS5PlfpV3ivmRp9lZHO6Ddu_7.jpeg",
      "alt": "Definitions and difference between \"Robots.txt\" and \"Meta Robots Tag\"."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/files/e3/c1/e3c11aed0d9bffb525e8f7552d6b7fa1/illustration-ads-banner-205x170.svg",
      "alt": "ADS illustration"
    },
    {
      "src": "https://data.adxcel-ec2.com/pixel/?ad_log=referer&action=content&pixid=1bc0716b-4a34-4511-bf3a-c999ecddd356",
      "alt": ""
    },
    {
      "src": "https://sp.analytics.yahoo.com/sp.pl?a=10000&d=Mon%2C%2021%20Oct%202024%2014%3A12%3A28%20GMT&n=4d&b=What%20Is%20Googlebot%3F%20How%20Google%E2%80%98s%20Web%20Crawler%20Works&.yp=10160379&f=https%3A%2F%2Fwww.semrush.com%2Fblog%2Fgooglebot%2F&enc=UTF-8&us_privacy=1---&yv=1.16.5&tagmgr=gtm",
      "alt": "dot image pixel"
    },
    {
      "src": "https://bat.bing.com/action/0?ti=5128787&tm=gtm002&Ver=2&mid=7d8e5846-1d23-4671-9d44-25db5f6a5180&bo=1&sid=806223e08fb611ef9e037323bfc6a30d&vid=80622b308fb611ef9c34e75903f77346&vids=0&msclkid=N&uach=pv%3D10.0.0&pi=0&lg=en-US&sw=1536&sh=864&sc=24&tl=What%20Is%20Googlebot%3F%20How%20Google%E2%80%98s%20Web%20Crawler%20Works&p=https%3A%2F%2Fwww.semrush.com%2Fblog%2Fgooglebot%2F&r=&lt=2087&evt=pageLoad&sv=1&cdb=ARoR&rn=127030",
      "alt": ""
    },
    {
      "src": "https://sp.analytics.yahoo.com/sp.pl?a=10000&b=What%20Is%20Googlebot%3F%20How%20Google%E2%80%98s%20Web%20Crawler%20Works&.yp=10160379&f=https%3A%2F%2Fwww.semrush.com%2Fblog%2Fgooglebot%2F&enc=UTF-8&us_privacy=1---&yv=1.16.5&tagmgr=gtm",
      "alt": "dot image pixel"
    },
    {
      "src": "https://data.adxcel-ec2.com/pixel/?ad_log=referer&action=content&pixid=1bc0716b-4a34-4511-bf3a-c999ecddd356",
      "alt": ""
    }
  ],
  "publication_date": null
}