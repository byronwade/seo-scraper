{
  "url": "https://www.semrush.com/blog/beginners-guide-robots-txt/",
  "title": "What Robots.Txt Is & Why It Matters for SEO",
  "content": "What Is a Robots.txt File?\n\nA robots.txt file is a set of instructions telling search engines which pages should and shouldn’t be crawled on a website. Which guides crawler access but shouldn’t be used to keep pages out of Google's index.\n\nA robots.txt file looks like this:\n\nRobots.txt files might seem complicated, but the syntax (computer language) is straightforward. \n\nBefore we get into those details, let’s offer some clarification on how robots.txt differs from some terms that sound similar. \n\nRobots.txt vs. Meta Robots vs. X-Robots\n\nRobots.txt files, meta robots tags, and x-robots tags all guide search engines about how to handle your site’s content.\n\nBut they differ in their level of control, where they’re located, and what they control. \n\nHere are the specifics: \n\nRobots.txt: This file is located in your website's root directory and acts as a gatekeeper to provide general, site-wide instructions to search engine crawlers on which areas of your site they should and shouldn’t crawl\nMeta robots tags: These are snippets of code that reside within the <head> section of individual webpages. And provide page-specific instructions to search engines on whether to index (include in search results) and follow (crawl links within) each page.\nX-robot tags: These are code snippets that are primarily used for non-HTML files like PDFs and images. And are implemented in the file's HTTP header.\n\nFurther reading: Meta Robots Tag & X-Robots-Tag Explained\n\nWhy Is Robots.txt Important for SEO? \n\nA robots.txt file helps manage web crawler activities, so they don’t overwork your website or bother with pages not meant for public view. \n\nBelow are a few reasons to use a robots.txt file:\n\n1. Optimize Crawl Budget\n\nCrawl budget refers to the number of pages Google will crawl on your site within a given time frame.\n\nThe number can vary based on your site’s size, health, and number of backlinks. \n\nIf your website’s number of pages exceeds your site’s crawl budget, you could have important pages that fail to get indexed. \n\nThose unindexed pages won’t rank. Meaning you wasted time creating pages users won’t see. \n\nBlocking unnecessary pages with robots.txt allows Googlebot (Google’s web crawler) to spend more crawl budget on pages that matter. \n\nNote\n\nMost website owners don’t need to worry too much about crawl budget, according to Google. This is primarily a concern for larger sites with thousands of URLs.\n\n2. Block Duplicate and Non-Public Pages\n\nCrawl bots don’t need to sift through every page on your site. Because not all of them were created to be served in the search engine results pages (SERPs).\n\nLike staging sites, internal search results pages, duplicate pages, or login pages. Some content management systems handle these internal pages for you.\n\nWordPress, for example, automatically disallows the login page “/wp-admin/” for all crawlers. \n\nRobots.txt allows you to block these pages from crawlers.\n\n3. Hide Resources\n\nSometimes, you want to exclude resources such as PDFs, videos, and images from search results. \n\nTo keep them private or have Google focus on more important content.\n\nIn either case, robots.txt keeps them from being crawled.\n\nHow Does a Robots.txt File Work?\n\nRobots.txt files tell search engine bots which URLs they should crawl and (more importantly) which ones to ignore.\n\nAs they crawl webpages, search engine bots discover and follow links. This process takes them from site A to site B to site C across links, pages, and websites. \n\nBut if a bot finds a robots.txt file, it will read it before doing anything else.\n\nThe syntax is straightforward. \n\nYou assign rules by identifying the “user-agent” (search engine bot) and specifying the directives (rules).\n\nYou can also use an asterisk (*) to assign directives to every user-agent, which applies the rule for all bots.\n\nFor example, the below instruction allows all bots except DuckDuckGo to crawl your site:\n\nNote\n\nAlthough a robots.txt file provides instructions, it can't enforce them. Think of it as a code of conduct. Good bots (like search engine bots) will follow the rules, but bad bots (like spam bots) will ignore them.\n\nSemrush bots crawl the web to gather insights for our website optimization tools, such as Site Audit, Backlink Audit, and On Page SEO Checker.\n\nOur bots respect the rules outlined in your robots.txt file. So, if you block our bots from crawling your website, they won’t.\n\nBut doing that also means you can’t use some of our tools to their full potential.\n\nFor example, if you blocked our SiteAuditBot from crawling your website, you couldn’t audit your site with our Site Audit tool. To analyze and fix technical issues on your site.\n\nIf you blocked our SemrushBot-SI from crawling your site, you couldn’t use the On Page SEO Checker tool effectively.\n\nAnd you’d lose out on generating optimization ideas to improve your webpages’ rankings.\n\nHow to Find a Robots.txt File\n\nYour robots.txt file is hosted on your server, just like any other file on your website.\n\nYou can view the robots.txt file for any given website by typing the full URL for the homepage and adding “/robots.txt” at the end.\n\nLike this: “https://semrush.com/robots.txt.”\n\nNote\n\nA robots.txt file should always live at the root domain level. For “www.example.com,” the robots.txt file lives at “www.example.com/robots.txt.” Place it anywhere else, and crawlers may assume you don’t have one. \n\nBefore learning how to create a robots.txt file or going into the syntax, let’s first look at some examples.\n\nExamples of Robots.txt Files\n\nHere are some real-world robots.txt examples from popular websites.\n\nYouTube\n\nYouTube’s robots.txt file tells crawlers not to access user comments, video feeds, login/signup pages, and age verification pages.\n\nThis discourages the indexing of user-specific or dynamic content that’s often irrelevant to search results and may raise privacy concerns.\n\nG2\n\nG2’s robots.txt file tells crawlers not to access sections with user-generated content. Like survey responses, comments, and contributor profiles.\n\nThis helps protect user privacy by protecting potentially sensitive personal information. And also prevents users from attempting to manipulate search results.\n\nNike\n\nNike’s robots.txt file uses the disallow directive to block crawlers from accessing user-generated directories. Like \"/checkout/\" and \"*/member/inbox.\" \n\nThis ensures that potentially sensitive user data isn’t exposed in search results. And prevents attempts to manipulate SEO rankings. \n\nSearch Engine Land\n\nSearch Engine Land’s robots.txt file uses the disallow tag to discourage the indexing of \"/tag/\" directory pages. Which tend to have low SEO value compared to actual content pages. And can cause duplicate content issues.\n\nThis encourages search engines to prioritize crawling higher-quality content, maximizing the website's crawl budget.\n\nWhich is especially important given how many pages Search Engine Land has.\n\nForbes\n\nForbes’s robots.txt file instructs Google to avoid the \"/test/\" directory. Which likely contains testing or staging environments.\n\nThis prevents unfinished or sensitive content from being indexed (assuming it isn’t linked to elsewhere.)\n\nExplaining Robots.txt Syntax\n\nA robots.txt file is made up of:\n\nOne or more blocks of “directives” (rules)\nEach with a specified “user-agent” (search engine bot)\nAnd an “allow” or “disallow” instruction\n\nA simple block can look like this:\n\nUser-agent: Googlebot\nDisallow: /not-for-google\nUser-agent: DuckDuckBot\nDisallow: /not-for-duckduckgo\nSitemap: https://www.yourwebsite.com/sitemap.xml\nThe User-Agent Directive\n\nThe first line of every directive block is the user-agent, which identifies the crawler.\n\nIf you want to tell Googlebot not to crawl your WordPress admin page, for example, your directive will start with:\n\nUser-agent: Googlebot\nDisallow: /wp-admin/\n\nNote\n\nMost search engines have multiple crawlers. They use different crawlers for standard indexing, images, videos, etc. \n\nWhen multiple directives are present, the bot may choose the most specific block of directives available. \n\nLet’s say you have three sets of directives: one for *, one for Googlebot, and one for Googlebot-Image. \n\nIf the Googlebot-News user agent crawls your site, it will follow the Googlebot directives. \n\nOn the other hand, the Googlebot-Image user agent will follow the more specific Googlebot-Image directives.\n\nThe Disallow Robots.txt Directive\n\nThe second line of a robots.txt directive is the “disallow” line.\n\nYou can have multiple disallow directives that specify which parts of your site the crawler can’t access. \n\nAn empty disallow line means you’re not disallowing anything—a crawler can access all sections of your site. \n\nFor example, if you wanted to allow all search engines to crawl your entire site, your block would look like this:\n\nUser-agent: *\nAllow: /\n\nIf you wanted to block all search engines from crawling your site, your block would look like this:\n\nUser-agent: *\nDisallow: /\n\nNote\n\nDirectives such as “Allow” and “Disallow” aren’t case-sensitive. But the values within each directive are. But you often find “Allow” and “Disallow” directives capitalized to make the file easier for humans to read.\n\nThe Allow Directive\n\nThe “allow” directive allows search engines to crawl a subdirectory or specific page, even in an otherwise disallowed directory.\n\nFor example, if you want to prevent Googlebot from accessing every post on your blog except for one, your directive might look like this:\n\nUser-agent: Googlebot\nDisallow: /blog\nAllow: /blog/example-post\n\nNote\n\nNot all search engines recognize this command. But Google and Bing support this directive.\n\nThe Sitemap Directive\n\nThe Sitemap directive tells search engines—specifically Bing, Yandex, and Google—where to find your XML sitemap.\n\nSitemaps generally include the pages you want search engines to crawl and index.\n\nThis directive lives at the top or bottom of a robots.txt file and looks like this:\n\nAdding a Sitemap directive to your robots.txt file is a quick alternative. But you can (and should) also submit your XML sitemap to each search engine using their webmaster tools.\n\nSearch engines will crawl your site eventually, but submitting a sitemap speeds up the crawling process. \n\nThe Crawl-Delay Directive\n\nThe “crawl-delay” directive instructs crawlers to delay their crawl rates. To avoid overtaxing a server (i.e., slowing down your website).\n\nGoogle no longer supports the crawl-delay directive. And if you want to set your crawl rate for Googlebot, you’ll have to do it in Search Console.\n\nBut Bing and Yandex do support the crawl-delay directive. Here’s how to use it.\n\nLet’s say you want a crawler to wait 10 seconds after each crawl action. You would set the delay to 10 like so:\n\nUser-agent: *\nCrawl-delay: 10\n\nFurther reading: 15 Crawlability Problems & How to Fix Them\n\nThe Noindex Directive\n\nA robots.txt file tells a bot what it should or shouldn’t crawl. But it can’t tell a search engine which URLs not to index and serve in search results.\n\nUsing the noindex tag in your robots.txt file may block a bot from knowing what’s on your page. But the page can still show up in search results. Albeit with no information. \n\nLike this:\n\nGoogle never officially supported this directive. And on September 1, 2019, Google even announced that they indeed don’t support the noindex directive in robots.txt.\n\nIf you want to reliably exclude a page or file from appearing in search results, avoid this directive altogether and use a meta robots noindex tag instead.\n\nHow to Create a Robots.txt File\n\nUse a robots.txt generator tool or create one yourself. \n\nHere’s how to create one from scratch:\n\n1. Create a File and Name It Robots.txt\n\nStart by opening a .txt document within a text editor or web browser. \n\nNote\n\nDon’t use a word processor, as they often save files in a proprietary format that can add random characters. \n\nNext, name the document “robots.txt.” \n\nYou’re now ready to start typing directives.\n\n2. Add Directives to the Robots.txt File\n\nA robots.txt file consists of one or more groups of directives. And each group consists of multiple lines of instructions.\n\nEach group begins with a user-agent and has the following information:\n\nWho the group applies to (the user-agent)\nWhich directories (pages) or files the agent should access\nWhich directories (pages) or files the agent shouldn’t access\nA sitemap (optional) to tell search engines which pages and files you deem important\n\nCrawlers ignore lines that don’t match these directives.\n\nLet’s say you don’t want Google crawling your “/clients/” directory because it’s just for internal use.\n\nThe first group would look something like this: \n\nUser-agent: Googlebot\nDisallow: /clients/\n\nAdditional instructions can be added in a separate line below, like this:\n\nUser-agent: Googlebot\nDisallow: /clients/\nDisallow: /not-for-google\n\nOnce you’re done with Google’s specific instructions, hit enter twice to create a new group of directives. \n\nLet’s make this one for all search engines and prevent them from crawling your “/archive/” and “/support/” directories because they’re for internal use only. \n\nIt would look like this:\n\nUser-agent: Googlebot\nDisallow: /clients/\nDisallow: /not-for-google\nUser-agent: *\nDisallow: /archive/\nDisallow: /support/\n\nOnce you’re finished, add your sitemap.\n\nYour finished robots.txt file would look something like this:\n\nUser-agent: Googlebot\nDisallow: /clients/\nDisallow: /not-for-google\nUser-agent: *\nDisallow: /archive/\nDisallow: /support/\nSitemap: https://www.yourwebsite.com/sitemap.xml\n\nThen, save your robots.txt file. And remember that it must be named “robots.txt.”\n\nNote\n\nCrawlers read from top to bottom and match with the first, most specific group of rules. So, start your robots.txt file with specific user agents first, and then move on to the more general wildcard (*) that matches all crawlers.\n\n3. Upload the Robots.txt File\n\nAfter you’ve saved the robots.txt file to your computer, upload it to your site and make it available for search engines to crawl.\n\nUnfortunately, there’s no universal tool for this step.\n\nUploading the robots.txt file depends on your site’s file structure and web hosting.\n\nSearch online or reach out to your hosting provider for help on uploading your robots.txt file.\n\nFor example, you can search for \"upload robots.txt file to WordPress.\"\n\nBelow are some articles explaining how to upload your robots.txt file in the most popular platforms:\n\nRobots.txt file in WordPress\nRobots.txt file in Wix\nRobots.txt file in Joomla\nRobots.txt file in Shopify\nRobots.txt file in BigCommerce\n\nAfter uploading the file, check if anyone can see it and if Google can read it.\n\nHere’s how.\n\n4. Test Your Robots.txt File\n\nFirst, test whether your robots.txt file is publicly accessible (i.e., if it was uploaded correctly).\n\nOpen a private window in your browser and search for your robots.txt file. \n\nFor example, “https://semrush.com/robots.txt.”\n\nIf you see your robots.txt file with the content you added, you’re ready to test the markup (HTML code). \n\nGoogle offers two options for testing robots.txt markup:\n\nThe robots.txt report in Search Console\nGoogle’s open-source robots.txt library (advanced)\n\nBecause the second option is geared toward advanced developers, let’s test with Search Console.\n\nNote\n\nYou must have a Search Console account set up to test your robots.txt file. \n\nGo to the robots.txt report by clicking the link.\n\nIf you haven’t linked your website to your Google Search Console account, you’ll need to add a property first.\n\nThen, verify that you’re the site’s owner.\n\nNote\n\nGoogle is planning to shut down this setup wizard. So in the future, you’ll have to directly verify your property in the Search Console. Read our full guide to Google Search Console to learn how.\n\nIf you have existing verified properties, select one from the drop-down list.\n\nThe tool will identify syntax warnings and logic errors. \n\nAnd display the total number of warnings and errors below the editor.\n\nYou can edit errors or warnings directly on the page and retest as you go.\n\nAny changes made on the page aren’t saved to your site. So, copy and paste the edited test copy into the robots.txt file on your site.\n\nSemrush’s Site Audit tool can also check for issues regarding your robots.txt file. \n\nFirst, set up a project in the tool to audit your website.\n\nOnce the audit is complete, navigate to the “Issues” tab and search for “robots.txt.”\n\nClick on the “Robots.txt file has format errors” link if it turns out that your file has format errors.\n\nYou’ll see a list of invalid lines.\n\nYou can click “Why and how to fix it” to get specific instructions on how to fix the error.\n\nChecking your robots.txt file for issues is important, as even minor mistakes can negatively affect your site’s indexability.\n\nFind and Fix Robots.txt Issues\n\nwith the Site Audit Tool\n\nSign Up Now →\nRobots.txt Best Practices\nUse a New Line for Each Directive \n\nEach directive should sit on a new line.\n\nOtherwise, search engines won’t be able to read them. And your instructions will be ignored. \n\nIncorrect:\n\nUser-agent: * Disallow: /admin/\nDisallow: /directory/\n\nCorrect:\n\nUser-agent: *\nDisallow: /admin/\nDisallow: /directory/\nUse Each User-Agent Only Once\n\nBots don’t mind if you enter the same user-agent multiple times.\n\nBut referencing it only once keeps things neat and simple. And reduces the chances of human error. \n\nConfusing:\n\nUser-agent: Googlebot\nDisallow: /example-page\nUser-agent: Googlebot\nDisallow: /example-page-2\n\nNotice how the Googlebot user-agent is listed twice?\n\nClear:\n\nUser-agent: Googlebot\nDisallow: /example-page\nDisallow: /example-page-2\n\nIn the first example, Google would still follow the instructions. But writing all directives under the same user-agent is cleaner and helps you stay organized.\n\nUse Wildcards to Clarify Directions\n\nYou can use wildcards (*) to apply a directive to all user-agents and match URL patterns. \n\nTo prevent search engines from accessing URLs with parameters, you could technically list them out one by one. \n\nBut that’s inefficient. You can simplify your directions with a wildcard.\n\nInefficient:\n\nUser-agent: *\nDisallow: /shoes/vans?\nDisallow: /shoes/nike?\nDisallow: /shoes/adidas?\n\nEfficient:\n\nUser-agent: *\nDisallow: /shoes/*?\n\nThe above example blocks all search engine bots from crawling all URLs under the “/shoes/” subfolder with a question mark.\n\nUse ‘$’ to Indicate the End of a URL\n\nAdding the “$” indicates the end of a URL. \n\nFor example, if you want to block search engines from crawling all .jpg files on your site, you can list them individually. \n\nBut that would be inefficient. \n\nInefficient:\n\nUser-agent: *\nDisallow: /photo-a.jpg\nDisallow: /photo-b.jpg\nDisallow: /photo-c.jpg\n\nInstead, add the “$” feature: \n\nEfficient:\n\nUser-agent: *\nDisallow: /*.jpg$\n\nNote​​​​​\n\nIn this example, “/dog.jpg” can’t be crawled, but “/dog.jpg?p=32414” can be because it doesn’t end with “.jpg.” \n\nThe “$” expression is a helpful feature in specific circumstances like above. But it can also be dangerous.\n\nYou can easily unblock things you didn’t mean to, so be prudent in its application.\n\nUse the Hash Symbol to Add Comments\n\nCrawlers ignore everything that starts with a hash (#). \n\nSo, developers often use a hash to add a comment in the robots.txt file. It helps keep the file organized and easy to read. \n\nTo add a comment, begin the line with a hash (#).\n\nLike this:\n\nUser-agent: *\n#Landing Pages\nDisallow: /landing/\nDisallow: /lp/\n#Files\nDisallow: /files/\nDisallow: /private-files/\n#Websites\nAllow: /website/*\nDisallow: /website/search/*\n\nDevelopers occasionally include funny messages in robots.txt files because they know users rarely see them.\n\nFor example, YouTube’s robots.txt file reads: “Created in the distant future (the year 2000) after the robotic uprising of the mid 90’s which wiped out all humans.”\n\nAnd Nike’s robots.txt reads “just crawl it” (a nod to its “just do it” tagline) and its logo.\n\nUse Separate Robots.txt Files for Different Subdomains \n\nRobots.txt files control crawling behavior only on the subdomain in which they’re hosted. \n\nTo control crawling on a different subdomain, you’ll need a separate robots.txt file. \n\nSo, if your main site lives on “domain.com” and your blog lives on the subdomain “blog.domain.com,” you’d need two robots.txt files. One for the main domain's root directory and the other for your blog’s root directory.\n\n5 Robots.txt Mistakes to Avoid\n\nWhen creating your robots.txt file, here are some common mistakes you should watch out for. \n\n1. Not Including Robots.txt in the Root Directory\n\nYour robots.txt file should always be located in your site's root directory. So that search engine crawlers can find your file easily.\n\nFor example, if your website is “www.example.com,” your robots.txt file should be located at \"www.example.com/robots.txt.\"\n\n If you put your robots.txt file in a subdirectory, such as \"www.example.com/contact/robots.txt,\" search engine crawlers may not find it. And may assume that you haven't set any crawling instructions for your website. \n\n2. Using Noindex Instructions in Robots.txt\n\nRobots.txt should focus on crawling directives, not indexing ones. Again, Google doesn’t support the noindex rule in the robots.txt file.\n\nInstead, use meta robots tags (e.g., <meta name=\"robots\" content=\"noindex\">) on individual pages to control indexing.\n\nLike so:\n\n3. Blocking JavaScript and CSS\n\nBe careful not to block search engines from accessing JavaScript and CSS files via robots.txt. Unless you have a specific reason for doing so, such as restricting access to sensitive data. \n\nBlocking search engines from crawling these files using your robots.txt can make it harder for those search engines to understand your site's structure and content.\n\nWhich can potentially harm your search rankings. Because search engines may not be able to fully render your pages.\n\nFurther reading: JavaScript SEO: How to Optimize JS for Search Engines\n\n4. Not Blocking Access to Your Unfinished Site or Pages\n\nWhen developing a new version of your site, you should use robots.txt to block search engines from finding it prematurely. To prevent unfinished content from being shown in search results.\n\nSearch engines crawling and indexing an in-development page can lead to poor user experience. And potential duplicate content issues.\n\nBy blocking access to your unfinished site with robots.txt, you ensure that only your site's final, polished version appears in search results.\n\nNote\n\nMake sure no links to your in-development pages exist online. Otherwise, they could still be indexed. \n\n5. Using Absolute URLs\n\nUse relative URLs in your robots.txt file to make it easier to manage and maintain.\n\nAbsolute URLs are unnecessary and can introduce errors if your domain changes.\n\n❌ Here’s an example of a robots.txt file with absolute URLs:\n\nUser-agent: *\nDisallow: https://www.example.com/private-directory/\nDisallow: https://www.example.com/temp/\nAllow: https://www.example.com/important-directory/\n\n✅ And one without:\n\nUser-agent: *\nDisallow: /private-directory/\nDisallow: /temp/\nAllow: /important-directory/\nKeep Your Robots.txt File Error-Free \n\nNow that you understand how robots.txt files work, it's important to optimize your own robots.txt file. Because even small mistakes can negatively impact your website's ability to be properly crawled, indexed, and displayed in search results.\n\nSemrush's Site Audit tool makes it easy to analyze your robots.txt file for errors and get actionable recommendations to fix any issues.\n\nFind and Fix Robots.txt Errors\n\nwith the Site Audit tool\n\nTry It For Free →",
  "headers": {
    "h1": "What Robots.Txt Is & Why It Matters for SEO",
    "h2": [
      "What Is a Robots.txt File?",
      "Robots.txt vs. Meta Robots vs. X-Robots",
      "Why Is Robots.txt Important for SEO? ",
      "How Does a Robots.txt File Work?",
      "How to Find a Robots.txt File",
      "Examples of Robots.txt Files",
      "Explaining Robots.txt Syntax",
      "How to Create a Robots.txt File",
      "Robots.txt Best Practices",
      "5 Robots.txt Mistakes to Avoid",
      "Keep Your Robots.txt File Error-Free "
    ]
  },
  "images": [
    {
      "src": "https://static.semrush.com/blog/uploads/media/f2/3c/f23c91aea6ef15f7f7d022e31fb5d912/guide-to-robots.txt.svg",
      "alt": "Robots.txt"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/db/ba/dbbae090b1a2be3f7a40bb650747e70a/1a361a453a195c0b7c8aa789dd3117a3/AD_4nXfS7ShfvmVLFVr5l4e7Kma2bhe3QHp5chu4snug1McDqeAdpYJOhL_qQk_NoDz0i6br0JmTL-bqckLwbVxvMiGiNhFdG62BPvjAZ1D8twGksu7eBpR09_umhA9Mladi2wGTPNCMOnJrVpP_TCI8AS-Rfjhr.png",
      "alt": "robots.txt example"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/4d/b3/4db38d26a9df16d7ef20d56f3d3a1137/01f61b014e3aca5ca08479bc1dea09a3/AD_4nXeexe3zz9Fdw07ruk1l48aZpKQiLVqdrMgvCiazr9fvlysZcdu8-w8jBQAZNIx8jKT9OHgQQo2CeAMJqfkUQlAM1_My77cGDgxy3DxiDQTDQ7KEq7x_cxGAD2wqXWjq6IB1YNYeqc6X43gdJ7Q7RoYlav8K.png",
      "alt": "all bots except DuckDuckGo instructed to crawl the site"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/73/7d/737d4fbe100805d0d7b0341c04598082/1030f1c4bda1b1d229a4f7aacfd0f96a/AD_4nXc59epZQzN04N7wIsG1yC1LN5y-O8J_vJV1nN-uwMH_OaKu25QpyEMFa1Iz18QBQiQGssBEeXcRS-65UcKb3y8sU026LJOVDmopEko-23hWlk27H1PBw08y1BF_Zt3vY5KQc4vTwI7GGEz7rzGWFSraEW8.png",
      "alt": "Errors section in Site Audit tool"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/81/0a/810a526b89c365fbdfe0e8f06b288f56/e2e8d8af1b94ae26d2e90534f78b8991/AD_4nXf2Hp-_lTuD5dmqR7yKQpzm0hEby3jF--JxbIzUIKKbGiA6m6rsmYOdzsrNc1g-HRqxprpNh47mZMYy2ym2JFco9e_gMSvwr-aTNIfcKldIE2cdoSRyl2b0oLteevNOq-82aaShaASEu7jki69_2qUudzhB.png",
      "alt": "On Page SEO Checker tool"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/9e/4d/9e4d4a6931f3e08f25875a2d48578bf7/731a101ecb9c54829cb3f4996450aeac/AD_4nXfRjiKfIglLXo5XV203PgIwXyfQTHNFnNJfJ8K3ebhAT-7d65bLmox--2Wk9gH9fHWOts88634810Q2cNV9pfWZsiQd1xKG7YMC_uuFG9uVK-3ZwrJaLUZfKVJv0SOtnElUyKuP13vxln50PUMVDCGTxK95.png",
      "alt": "https://semrush.com/robots.txt example"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/2a/d9/2ad908b804b7c40fd2971555a9d496a9/79c6a13799b2ef85d028b1dec18dff5b/AD_4nXf2i9h_ruWgT8x-91863D_m4n13x8E1wzG5DvsxSEiqECBdP-DPKJRWBntUgQffkvUso18Ivg-OyGJkhGc2Ow7Kn9La8YVQ8BzeVYdBhpBbt2LxHBl-4aG4Q0zbXe70AGcgsM7V9jbHEAVm5th4mnroxt8.png",
      "alt": "YouTube robots.txt file with highlights showing disallowed URL paths."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/dc/ff/dcff864419df021a16a7f596de98aaf8/e1738793aa6518d8f9355267483d4745/AD_4nXfZ8GTC2lrthmXFFvJvLhCqsmRVz7OR_dfdknr4nQgreRCS8Do_GsWUtImXUHtBcZ_BqwRMGcX6ZUmn1od-bOjATqAJLk0DPF_by2t2EIBEuF_yrNURWh2h1iYz3piY4xX9fr68C-MtrpiHps7qlzDWIxA9.png",
      "alt": "G2 robots.txt file with highlights showing disallowed URL paths."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/77/d7/77d72dd382272217dd2578dc1cb3de35/97887fd8c9b65751971d8112901a645c/AD_4nXfAXIJIVcmSm_f2QXV7NvvhLyB1C5geuP8WShevUL7BijsdcQfa4RyopFPfXSAzXdsVbc7cAKbfx2Ep0IEABu0irokUg-rkA4R_XZuHB-P1LJ2oKGLxp0O8UjamYTt9eIGEX88LLZwPAUtB75WWd2hV2w0m.png",
      "alt": "Nike robots.txt file with highlights showing disallowed URL paths."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/c6/3c/c63cbbcb6240c08742f2294507b720e4/4d658a65874f824caea3ce5c5fb24bcf/AD_4nXcmLgZcMaTdHeZADI07AsaUbbxUAUNjC_RsCn2F1y5g9lKzJB32wh3-HD-SV7QA-O-T9Sn9Y8sdvlnyJu9fywiJP9IPKEXzyThbzvQOxyYOtn-7lKeI-DEU9EGV4mRIChfoO5wv_2wkXhq0qDWqZrNUb0MZ.png",
      "alt": "Search Engine Land robots.txt file with highlight showing disallowed URL path."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/01/3a/013adb1363d70384f42bc54a5654fd73/a912c5a25b8b6bbc93be28eb96df177e/AD_4nXeCikC_oLPvaZIJoBgsI7VHKsxfKmuVb8_z8MZXjjEaYBFQr7MVHVdnMt7QyOyqMG1pBQPCSvU2L5V7Bmf_ApM4HE6zNSpb5ZJrDBSoApaJulyx29UiO9WEQhwcczyV4fZf23u4uTz603sxATad5NKR2FbA.png",
      "alt": "Forbes robots.txt file with highlight showing disallowed URL path."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/c4/2a/c42a2491d23c5eea0c013f1f8b074067/8dfdc7e25383a3e0b63f941220f37f36/AD_4nXeQACe7VzqB3U56MFBadoQVXnUzJh0ZnRirtCX48M0Wkm6efbjX6AUNhVWkdhUTKJWZMLSUKuemjMAruLn8wgkkVYQ9hNrOfrQLo1OTT8uIw3L6yi21CdiUc_DtldBUqX2OxgIBvaMUjIEXc30GYz6AO7I.png",
      "alt": "The Sitemap directive example"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/9c/71/9c71278a1656f2eee886d7ffa5abe7b9/26c3ae980a1787721ab177252a3745a7/AD_4nXdqkV3jktP3GusYMhdh8KJId674jlZEPflT-vOKP_92F9RerP9rQcvHLciKJuNzESW9dYxCr30FvgkLk6faymcIY8qlgElSWH5WTfYg-bHrzDehq_41i3uDkBIcWqeezHDJJd6aGarnK-oezU601ObxSeOP.png",
      "alt": "Example of page indexed and blocked by robots.txt in search results"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/e0/39/e039e6d2fc56491f15bfc1186b63b141/58ab7336612ff2e7aaf7612478798ee3/AD_4nXcN2LEgq0eIMN9JX4TNUV1-jYt6Se_XVD0SQ8xIJLPZoZITTTHi5BwuN4y96s5qEmcRKDEPrIYJoG_MrLMNwh4GSRMpqhI07KnEl2PTriquJkS4f4XsVUGuqH-QiNhJibhDUPqYubGYD9ILoObSNjE5_t4N.png",
      "alt": "Example of https://semrush.com/robots.txt search"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/9d/5d/9d5dbb2c323d4c03e4b9ba6d038d7b8e/fe98f48db08bff388d3b11a5b1d73a4c/AD_4nXe0NU6r2y1ZtC9enaUmiYKV6fDJcaIjWJqnC2sSyuDPqeQYMgHQqy8MU6qOmvMDb2A6snuX0qZRPRE6GYXwtfqAx3H1wX0Y31nakKCC5UApcQEQ_fpL-HwyUHPoBBHG2f2X1ull4Y9bEhq_kEg7m7oqglQ.png",
      "alt": "Add a property option shown"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/87/e4/87e44b28916a72ebddb6178b47206c27/125d3a1a02ab6d682d134264b8f9c77f/AD_4nXcp0Zj1yvADtwyqS93DezmIvAD0EXRpmobg4jP_4pWpGQVS3HQ8lGMJKpv5Tc9rKhto6Lz2valgNdmQvfOzVncvyOqsOU7-KR0rd-hoOkAP-ZNFjywa5-q9a2SfOUumxClCG4eqoS2F--fL-MXYkWtlu23U.png",
      "alt": "Verification step recommended method"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/36/83/36836201dc08c0b390f50ae14cf14937/a4d612217cacfcde6419fb91c3039de6/AD_4nXcmdKXNs52KKmre-Qn5F2nxOzWqNrKfRs02D-fZ-Yg9L3bTqK2vN0ne3SCM9kCUtM4jqHXQlqe7Dx4wHKPb9fN9VEEwFXQAlK9izkuP4muYYDw5R9oqGU6FhMZVyxufN0TlKgW9n3YCJDTCMELQ6iIIjYKz.png",
      "alt": "Display of warnings and errors"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/25/b7/25b782850c4ae9846a0a1a713b72b520/1e8ff3e50c78bcfd07ca67e0400b1f39/AD_4nXeXU-Kcrpl2T1xV7VXpfhFiAc6QphOeIWmffBiKQTo2gav-uLb4tbp3byaH3ErHvaBTdTNr9_PZb_hAZNva7dteBzZtYjrUSSGQGioOv6uAV-s7rq17vZyle-ZAPRxj22P4CGobynruwzOzlDTF2VD9g_kC.png",
      "alt": "Search for “robots.txt” in Site Audit"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/39/73/397398cb3880d93d7ffd465bdd69a808/5673fb9c9fbb14c44913f9517bb80edf/AD_4nXcNax2PHPkJPj9Y76HQuaXG_EO6m1812QvVuHWg3ujnd80OacsMMPEOsK5Bz_cerNsjnlXTDXhAeVzlBX_F1g2I5uvE0XvW7PcM1XLcb9sB9qh9iGPxhM2tLnikuYtTI0XKTW8BCFBbtsa07Tl4DcBkdZU.png",
      "alt": "“Robots.txt file has format errors”"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/4a/91/4a914c2245a7ab4285742138ec27e3d1/196736765199627a379fdb5bbd5a29d3/AD_4nXfIysCCWeDMIObD0Nnt-FuyMCj0b3-xRao8l-h17Qiimohq0GMoLQ0lOenGu04Em4kgRQiQ9vBprS5_MMLD7iXrtpA-KXDHxfy1Jpd4MUdi6U8OEbBEr-sjU59srFtr23HbG6fpvMlgA26R9vczDZ0T0DgX.png",
      "alt": "List of invalid lines"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/df/15/df1551f735b0a061a75e044a3d843910/7b1c0d723cc752b21e07f44e24751655/AD_4nXejtlKWI11qe5bFn-qrVk4RPUh_VvBVFQRp_iYY_SfwB6Zc19H699aEzaHhqdcRp-39y95QfDonYseVxcYWME39W-Rqg9FO1tlv9_rRwljYA1aPabj2eCrbCdAbWzW2v_UzDXZ2_bfgqTVQ6Fvr9_Q_qLX5.png",
      "alt": "“Why and how to fix it” section"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/files/e3/c1/e3c11aed0d9bffb525e8f7552d6b7fa1/illustration-ads-banner-205x170.svg",
      "alt": "ADS illustration"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/6a/e3/6ae3eceaa4bbf1b9bc774406ed2eedac/c5dd04e1aeb0ffb704971fc6ae81a22c/AD_4nXcrwdfz6x4kFi3zUqwUmUYf3uYfbv9FkGa35K3Xh4vcq6NZA9N-h839z0Lgq32Vuoad0OSOD48AUH6d4uvWTdkUQvp-r05BoLfaRFTv59arYPf_znXSCTwcNlo4Pn9g2yPncZtwJBa987bKMrYiCFCUIbOK.png",
      "alt": "YouTube’s robots.txt file example"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/21/b1/21b1cd8f032958f3493a258919a602ec/23b24d811bfe22a79c61b7a1523b0aab/AD_4nXfHSpx2yDD6um4syARzDjiW2QG3jp-SWKLCNMReVhlsHcVkVutkqdGMYt15q65VrR3pYT8cTvkqIlLa1n7IBCBUDurE8WY4xoHVPy6E2JzVb96ElRfSJ51c3SGyal6hxyBgM7ZYY26RD_iVuyMWBQeVfTcD.png",
      "alt": "Nike’s robots.txt example"
    },
    {
      "src": "https://static.semrush.com/blog/uploads/media/7b/2e/7b2e81cdc4c5cc7bb5e19b88069e7add/8aa1f2ebe433f880d396e722f84c598a/AD_4nXdZF0rd8bLNim-jcrD6qZfdKQRJGOzR632ikQwZIP3hXkbSAaWICWYBOBThJW3jqPeSyQ8ewBt5HQnRZElji7KOcXnL_ljlvfhNPXStTrqOzBsZgY_cR7J58gRfXqdDgSbAo2Z_J5se3H19mTnbyUnnEjr2.png",
      "alt": "noindex meta robots tag in page source code."
    },
    {
      "src": "https://static.semrush.com/blog/uploads/files/e3/c1/e3c11aed0d9bffb525e8f7552d6b7fa1/illustration-ads-banner-205x170.svg",
      "alt": "ADS illustration"
    },
    {
      "src": "https://static.semrush.com/semblog-next-static/banners/trial-gift.png",
      "alt": "Trial Semrush banner"
    },
    {
      "src": "https://data.adxcel-ec2.com/pixel/?ad_log=referer&action=content&pixid=1bc0716b-4a34-4511-bf3a-c999ecddd356",
      "alt": ""
    },
    {
      "src": "https://data.adxcel-ec2.com/pixel/?ad_log=referer&action=content&pixid=1bc0716b-4a34-4511-bf3a-c999ecddd356",
      "alt": ""
    },
    {
      "src": "https://sp.analytics.yahoo.com/sp.pl?a=10000&d=Mon%2C%2021%20Oct%202024%2014%3A12%3A44%20GMT&n=4d&b=What%20Robots.Txt%20Is%20%26%20Why%20It%20Matters%20for%20SEO&.yp=10160379&f=https%3A%2F%2Fwww.semrush.com%2Fblog%2Fbeginners-guide-robots-txt%2F&enc=UTF-8&us_privacy=1yn-&yv=1.16.5&tagmgr=gtm",
      "alt": "dot image pixel"
    },
    {
      "src": "https://sp.analytics.yahoo.com/sp.pl?a=10000&b=What%20Robots.Txt%20Is%20%26%20Why%20It%20Matters%20for%20SEO&.yp=10160379&f=https%3A%2F%2Fwww.semrush.com%2Fblog%2Fbeginners-guide-robots-txt%2F&enc=UTF-8&us_privacy=1yn-&yv=1.16.5&tagmgr=gtm",
      "alt": "dot image pixel"
    },
    {
      "src": "https://bat.bing.com/action/0?ti=5128787&tm=gtm002&Ver=2&mid=44947b03-df71-46a6-8b3a-cda477af7116&bo=1&sid=806223e08fb611ef9e037323bfc6a30d&vid=80622b308fb611ef9c34e75903f77346&vids=0&msclkid=N&uach=pv%3D10.0.0&pi=0&lg=en-US&sw=1536&sh=864&sc=24&tl=What%20Robots.Txt%20Is%20%26%20Why%20It%20Matters%20for%20SEO&p=https%3A%2F%2Fwww.semrush.com%2Fblog%2Fbeginners-guide-robots-txt%2F&r=&lt=651&evt=pageLoad&sv=1&cdb=ARoR&rn=706032",
      "alt": ""
    }
  ],
  "publication_date": null
}